{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('GSPC_2015_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    1259.000000\n",
       " mean        0.000538\n",
       " std         0.012115\n",
       " min        -0.119841\n",
       " 25%        -0.002932\n",
       " 50%         0.000648\n",
       " 75%         0.005094\n",
       " max         0.093828\n",
       " Name: Adj Close, dtype: float64,\n",
       " 0)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spx = df.set_index('Date').pct_change()['Adj Close'].fillna(0)\n",
    "spx.describe(), spx.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2015-08-24   -0.044432\n",
       "2015-08-25   -1.160540\n",
       "2015-08-26    3.177429\n",
       "2015-08-27    1.961108\n",
       "2015-08-28    0.005813\n",
       "                ...   \n",
       "2019-08-14   -2.462261\n",
       "2019-08-15    0.158969\n",
       "2019-08-16    1.146303\n",
       "2019-08-19    0.954788\n",
       "2019-08-20   -0.697715\n",
       "Name: Adj Close, Length: 1005, dtype: float64"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen_df.loc[:'2019-08-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 30)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "train_gen_df = (spx.loc[:'2019-08-20'] - spx.loc[:'2019-08-20'].mean()) / spx.loc[:'2019-08-20'].std()\n",
    "test_gen_df = (spx.loc['2019-08-20':] - spx.loc[:'2019-08-20'].mean()) / spx.loc[:'2019-08-20'].std()\n",
    "train_data_gen = TimeseriesGenerator(train_gen_df.values, train_gen_df, length=30)\n",
    "test_data_gen = TimeseriesGenerator(test_gen_df.values, test_gen_df, length=30)\n",
    "train_data_gen[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/usernaamee/keras-wavenet/blob/master/simple-generative-model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Convolution1D, Convolution2D, Flatten, Dense, \\\n",
    "    Input, Lambda, Activation, BatchNormalization\n",
    "from tensorflow import keras\n",
    "import tensorflow\n",
    "\n",
    "def WaveNetBlock(filters, kernel_size, dilation_rate, activation=None):\n",
    "    def f(input_):\n",
    "        residual = input_\n",
    "        tanh_out = Convolution1D(filters, kernel_size,\n",
    "                                       dilation_rate=dilation_rate,\n",
    "                                       padding='same',\n",
    "                                       activation='tanh')(input_)\n",
    "        sigmoid_out = Convolution1D(filters, kernel_size,\n",
    "                                          dilation_rate=dilation_rate,\n",
    "                                          padding='same',\n",
    "                                          activation='sigmoid')(input_)\n",
    "        merged = keras.layers.Multiply()([tanh_out, sigmoid_out])\n",
    "        skip_out = Convolution1D(1, 1, activation=activation, padding='same')(merged)\n",
    "        out = keras.layers.Add()([skip_out, residual])\n",
    "#         return out, skip_out\n",
    "        return skip_out\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    total_mae = mean_absolute_error(train_gen_df[30:], np.zeros_like(train_gen_df[30:]))\n",
    "    model_mae = mean_absolute_error(np.squeeze(model.predict(train_data_gen)), train_gen_df[30:]) \n",
    "    print('train', model_mae, total_mae, model_mae/total_mae)\n",
    "    total_mae = mean_absolute_error(test_gen_df[30:], np.zeros_like(test_gen_df[30:]))\n",
    "    model_mae = mean_absolute_error(np.squeeze(model.predict(test_data_gen)), test_gen_df[30:]) \n",
    "    print('test', model_mae, total_mae, model_mae/total_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = i = Input(shape=(30, 1))\n",
    "x = BatchNormalization()(x)\n",
    "# x = WaveNetBlock(filters=4, kernel_size=8, dilation_rate=1, activation='relu')(x)\n",
    "# x = WaveNetBlock(filters=8, kernel_size=4, dilation_rate=2, activation='relu')(x)\n",
    "# x = WaveNetBlock(filters=16, kernel_size=2, dilation_rate=4, activation='relu')(x)\n",
    "# x = WaveNetBlock(filters=32, kernel_size=1, dilation_rate=8, activation='relu')(x)\n",
    "x = Convolution1D(filters=1, kernel_size=1, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=1, kernel_size=1, dilation_rate=2, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=1, kernel_size=1, dilation_rate=4, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=1, kernel_size=1, dilation_rate=8, padding='same', activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "# x = Dense(30, activation='relu', kernel_regularizer='l2')(x)\n",
    "# x = Dense(30, activation='relu', kernel_regularizer='l2')(x)\n",
    "x = Dense(1, activation='linear', kernel_regularizer='l2')(x)\n",
    "\n",
    "\n",
    "model = keras.Model(i, x)\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-3), loss='mae')\n",
    "hist = model.fit(train_data_gen, epochs=300, validation_data=test_data_gen)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = WaveNetBlock(filters=4, kernel_size=8, dilation_rate=1, activation='relu')(x)\n",
    "# x = WaveNetBlock(filters=8, kernel_size=4, dilation_rate=2, activation='relu')(x)\n",
    "# x = WaveNetBlock(filters=16, kernel_size=2, dilation_rate=4, activation='relu')(x)\n",
    "# x = WaveNetBlock(filters=32, kernel_size=1, dilation_rate=8, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "8/8 [==============================] - 1s 130ms/step - loss: 1.1940 - val_loss: 1.9721\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.0646 - val_loss: 1.8476\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.9595 - val_loss: 1.7551\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.8772 - val_loss: 1.6872\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.8169 - val_loss: 1.6427\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7728 - val_loss: 1.6014\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7395 - val_loss: 1.5715\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.7161 - val_loss: 1.5468\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.7011 - val_loss: 1.5269\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6921 - val_loss: 1.5670\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6784 - val_loss: 1.5221\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6693 - val_loss: 1.5568\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6617 - val_loss: 1.5480\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6578 - val_loss: 1.5716\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.6568 - val_loss: 1.5732\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.6487 - val_loss: 1.5233\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.6405 - val_loss: 1.5421\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6346 - val_loss: 1.5760\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6392 - val_loss: 1.5236\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6274 - val_loss: 1.5541\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6244 - val_loss: 1.5237\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6199 - val_loss: 1.5255\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6152 - val_loss: 1.5546\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6090 - val_loss: 1.5564\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6010 - val_loss: 1.5354\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.5977 - val_loss: 1.5463\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5926 - val_loss: 1.5316\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5949 - val_loss: 1.5612\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5837 - val_loss: 1.6068\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5781 - val_loss: 1.5848\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.5663 - val_loss: 1.5585\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.5579 - val_loss: 1.5689\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.5528 - val_loss: 1.6090\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.5492 - val_loss: 1.5998\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5443 - val_loss: 1.6339\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.5571 - val_loss: 1.6138\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.5439 - val_loss: 1.6325\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.5304 - val_loss: 1.6140\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.5186 - val_loss: 1.6596\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5174 - val_loss: 1.6182\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.5067 - val_loss: 1.6723\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5094 - val_loss: 1.6332\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.5145 - val_loss: 1.5977\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5106 - val_loss: 1.7072\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.4972 - val_loss: 1.6459\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.5170 - val_loss: 1.6693\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4999 - val_loss: 1.6227\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4820 - val_loss: 1.7168\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.5255 - val_loss: 1.7771\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.5049 - val_loss: 1.6313\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.4792 - val_loss: 1.6161\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.5040 - val_loss: 1.6467\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5011 - val_loss: 1.6637\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4616 - val_loss: 1.6800\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4665 - val_loss: 1.6316\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4584 - val_loss: 1.6490\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.4661 - val_loss: 1.6684\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4813 - val_loss: 1.6840\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4477 - val_loss: 1.7120\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4340 - val_loss: 1.6610\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4555 - val_loss: 1.7056\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4188 - val_loss: 1.7110\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.4253 - val_loss: 1.7148\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4177 - val_loss: 1.6873\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4082 - val_loss: 1.7616\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4339 - val_loss: 1.6750\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4147 - val_loss: 1.6773\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4217 - val_loss: 1.8879\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4621 - val_loss: 1.7411\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4070 - val_loss: 1.7125\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.4295 - val_loss: 1.6879\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3969 - val_loss: 1.6945\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3905 - val_loss: 1.6871\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3857 - val_loss: 1.7892\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.3798 - val_loss: 1.7113\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3947 - val_loss: 1.7532\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.3763 - val_loss: 1.7175\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3630 - val_loss: 1.6867\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.3639 - val_loss: 1.7881\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.3588 - val_loss: 1.7597\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3487 - val_loss: 1.7414\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3295 - val_loss: 1.7323\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3296 - val_loss: 1.7765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3339 - val_loss: 1.7670\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3440 - val_loss: 1.7889\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.3336 - val_loss: 1.8215\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3560 - val_loss: 1.7688\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3343 - val_loss: 1.8005\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.3207 - val_loss: 1.7826\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.3178 - val_loss: 1.8186\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3145 - val_loss: 1.7793\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3220 - val_loss: 1.8250\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3131 - val_loss: 1.7495\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3033 - val_loss: 1.8151\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2943 - val_loss: 1.7659\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3576 - val_loss: 1.9001\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3399 - val_loss: 1.7812\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3285 - val_loss: 1.9045\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3462 - val_loss: 1.7526\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.3268 - val_loss: 1.8191\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3103 - val_loss: 1.8358\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2780 - val_loss: 1.8057\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2779 - val_loss: 1.8242\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2867 - val_loss: 1.8422\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2959 - val_loss: 1.9133\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3155 - val_loss: 1.8244\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.3385 - val_loss: 1.8131\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2867 - val_loss: 1.8835\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2735 - val_loss: 1.8044\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2781 - val_loss: 1.8002\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.3201 - val_loss: 1.8315\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.2717 - val_loss: 1.8245\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2647 - val_loss: 1.9041\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2727 - val_loss: 1.8196\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2528 - val_loss: 1.8932\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2509 - val_loss: 1.8253\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2756 - val_loss: 1.8081\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2817 - val_loss: 1.9092\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2946 - val_loss: 1.8319\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2875 - val_loss: 1.9371\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2883 - val_loss: 1.8067\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2991 - val_loss: 1.9737\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.2815 - val_loss: 1.8444\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.2473 - val_loss: 1.8508\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2751 - val_loss: 1.8925\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2944 - val_loss: 1.7967\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2687 - val_loss: 1.8363\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2946 - val_loss: 1.9808\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4014 - val_loss: 1.9392\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.4008 - val_loss: 1.7874\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.3307 - val_loss: 1.7953\n"
     ]
    }
   ],
   "source": [
    "x = i = Input(shape=(30, 1))\n",
    "x = BatchNormalization()(x)\n",
    "x = Convolution1D(filters=4, kernel_size=8, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=8, kernel_size=4, dilation_rate=2, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=16, kernel_size=2, dilation_rate=4, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=32, kernel_size=1, dilation_rate=8, padding='same', activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(30, activation='relu', kernel_regularizer='l2')(x)\n",
    "# x = Dense(30, activation='relu', kernel_regularizer='l2')(x)\n",
    "x = Dense(1, activation='linear', kernel_regularizer='l2')(x)\n",
    "\n",
    "\n",
    "model = keras.Model(i, x)\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-3), loss='mae')\n",
    "hist = model.fit(train_data_gen, epochs=300, validation_data=test_data_gen)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.6560850523329298 0.6542154487460116 1.002857779635901\n",
      "test 1.5009530248193332 1.4872528866796857 1.0092117072102198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c2d8afdec8>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = i = Input(shape=(30, 1))\n",
    "x = BatchNormalization()(x)\n",
    "x = Convolution1D(filters=4, kernel_size=8, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=8, kernel_size=4, dilation_rate=2, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=16, kernel_size=2, dilation_rate=4, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=32, kernel_size=1, dilation_rate=8, padding='same', activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(30, activation='relu', kernel_regularizer='l2')(x)\n",
    "# x = Dense(30, activation='relu', kernel_regularizer='l2')(x)\n",
    "x = Dense(1, activation='linear', kernel_regularizer='l2')(x)\n",
    "\n",
    "\n",
    "model = keras.Model(i, x)\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-3), loss='mae')\n",
    "hist = model.fit(train_data_gen, epochs=300, validation_data=test_data_gen)\n",
    "evaluate_model(model)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "8/8 [==============================] - 1s 114ms/step - loss: 0.6579 - val_loss: 1.4825\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.6476 - val_loss: 1.4814\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6403 - val_loss: 1.4857\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6349 - val_loss: 1.4826\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.6276 - val_loss: 1.4865\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6207 - val_loss: 1.4828\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.6112 - val_loss: 1.4715\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.6011 - val_loss: 1.4767\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.5907 - val_loss: 1.4562\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.5755 - val_loss: 1.4626\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5622 - val_loss: 1.4547\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5505 - val_loss: 1.4541\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5352 - val_loss: 1.4567\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.5172 - val_loss: 1.4341\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5165 - val_loss: 1.4368\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5004 - val_loss: 1.4294\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4869 - val_loss: 1.4385\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.4631 - val_loss: 1.4432\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4523 - val_loss: 1.4441\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.4468 - val_loss: 1.4533\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4322 - val_loss: 1.4453\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4265 - val_loss: 1.4427\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.4143 - val_loss: 1.4530\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.4547 - val_loss: 1.4241\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.4494 - val_loss: 1.4493\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4243 - val_loss: 1.4315\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4066 - val_loss: 1.4308\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3965 - val_loss: 1.4248\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.4083 - val_loss: 1.4605\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3610 - val_loss: 1.4547\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.3401 - val_loss: 1.4425\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3262 - val_loss: 1.4551\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.3135 - val_loss: 1.4637\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.3080 - val_loss: 1.4684\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2993 - val_loss: 1.4760\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3104 - val_loss: 1.4564\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2934 - val_loss: 1.4967\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2848 - val_loss: 1.4590\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2674 - val_loss: 1.4983\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2611 - val_loss: 1.4694\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2580 - val_loss: 1.4901\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2467 - val_loss: 1.4882\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2323 - val_loss: 1.4995\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2253 - val_loss: 1.4891\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2284 - val_loss: 1.4848\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2228 - val_loss: 1.5323\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2225 - val_loss: 1.5148\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2429 - val_loss: 1.5251\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2486 - val_loss: 1.5035\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.2583 - val_loss: 1.5365\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2545 - val_loss: 1.5408\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.2257 - val_loss: 1.5233\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.1967 - val_loss: 1.5322\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.1965 - val_loss: 1.5106\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1941 - val_loss: 1.5336\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2218 - val_loss: 1.5248\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1847 - val_loss: 1.5530\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.1758 - val_loss: 1.5401\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.1573 - val_loss: 1.5417\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1522 - val_loss: 1.5458\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1524 - val_loss: 1.5669\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1648 - val_loss: 1.5415\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1672 - val_loss: 1.5696\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1658 - val_loss: 1.5729\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1838 - val_loss: 1.5337\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1754 - val_loss: 1.5560\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1480 - val_loss: 1.5624\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1284 - val_loss: 1.5609\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1255 - val_loss: 1.5772\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1193 - val_loss: 1.5676\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1335 - val_loss: 1.5722\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1241 - val_loss: 1.5716\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1193 - val_loss: 1.5817\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1224 - val_loss: 1.5712\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1141 - val_loss: 1.5735\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.1268 - val_loss: 1.5798\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1426 - val_loss: 1.5708\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1423 - val_loss: 1.5847\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.1344 - val_loss: 1.5823\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1388 - val_loss: 1.5837\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.1598 - val_loss: 1.5803\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1556 - val_loss: 1.5723\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.1838 - val_loss: 1.5796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2121 - val_loss: 1.5427\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1778 - val_loss: 1.5565\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1302 - val_loss: 1.5752\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1342 - val_loss: 1.5820\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1086 - val_loss: 1.5669\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1037 - val_loss: 1.5804\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0999 - val_loss: 1.5751\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0894 - val_loss: 1.5869\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0894 - val_loss: 1.5924\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0962 - val_loss: 1.5790\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0964 - val_loss: 1.5679\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0924 - val_loss: 1.5807\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0880 - val_loss: 1.5609\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0913 - val_loss: 1.5852\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0904 - val_loss: 1.5797\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0863 - val_loss: 1.5869\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0978 - val_loss: 1.5947\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0826 - val_loss: 1.5830\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0828 - val_loss: 1.5884\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0824 - val_loss: 1.5873\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0838 - val_loss: 1.5852\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0891 - val_loss: 1.5742\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0875 - val_loss: 1.5789\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0847 - val_loss: 1.5759\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1087 - val_loss: 1.5876\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.1078 - val_loss: 1.5687\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1126 - val_loss: 1.5821\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1006 - val_loss: 1.5790\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.1045 - val_loss: 1.5898\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1093 - val_loss: 1.5926\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.1000 - val_loss: 1.5814\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0957 - val_loss: 1.5735\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0845 - val_loss: 1.5773\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0838 - val_loss: 1.5602\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0895 - val_loss: 1.5568\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1007 - val_loss: 1.5773\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0995 - val_loss: 1.5709\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0792 - val_loss: 1.5906\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0852 - val_loss: 1.5804\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1037 - val_loss: 1.5839\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.1123 - val_loss: 1.5728\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1073 - val_loss: 1.5782\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1024 - val_loss: 1.5843\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1061 - val_loss: 1.5726\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0873 - val_loss: 1.5967\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0852 - val_loss: 1.5920\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1012 - val_loss: 1.6015\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0899 - val_loss: 1.5804\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0723 - val_loss: 1.5952\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0765 - val_loss: 1.5896\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0716 - val_loss: 1.5943\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0725 - val_loss: 1.5854\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0721 - val_loss: 1.5872\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0677 - val_loss: 1.5880\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0638 - val_loss: 1.5776\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0753 - val_loss: 1.5914\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0620 - val_loss: 1.5887\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0685 - val_loss: 1.5919\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0657 - val_loss: 1.5931\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0664 - val_loss: 1.5949\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0716 - val_loss: 1.5929\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0692 - val_loss: 1.5810\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0756 - val_loss: 1.5816\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0646 - val_loss: 1.5791\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0695 - val_loss: 1.5865\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0745 - val_loss: 1.5812\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0773 - val_loss: 1.5843\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0567 - val_loss: 1.5870\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0702 - val_loss: 1.5817\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0726 - val_loss: 1.5965\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0810 - val_loss: 1.5876\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.0639 - val_loss: 1.5861\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.0685 - val_loss: 1.5780\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0802 - val_loss: 1.5769\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.0651 - val_loss: 1.5828\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0532 - val_loss: 1.5879\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0512 - val_loss: 1.5842\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0571 - val_loss: 1.5823\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0592 - val_loss: 1.5766\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0560 - val_loss: 1.5842\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0587 - val_loss: 1.5800\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0613 - val_loss: 1.5853\n",
      "Epoch 166/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0549 - val_loss: 1.5671\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0602 - val_loss: 1.5817\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0542 - val_loss: 1.5759\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0570 - val_loss: 1.5838\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0522 - val_loss: 1.5818\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.0581 - val_loss: 1.5846\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0578 - val_loss: 1.5837\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0639 - val_loss: 1.5762\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0766 - val_loss: 1.5754\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0730 - val_loss: 1.5765\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0692 - val_loss: 1.5769\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0639 - val_loss: 1.5708\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0872 - val_loss: 1.5670\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0860 - val_loss: 1.5729\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0803 - val_loss: 1.5744\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1006 - val_loss: 1.5883\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0707 - val_loss: 1.5773\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0765 - val_loss: 1.5882\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0806 - val_loss: 1.5829\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.0665 - val_loss: 1.5849\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0642 - val_loss: 1.5702\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0635 - val_loss: 1.5730\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0594 - val_loss: 1.5775\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0559 - val_loss: 1.5700\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0597 - val_loss: 1.5744\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0464 - val_loss: 1.5886\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0454 - val_loss: 1.5851\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0476 - val_loss: 1.5804\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0464 - val_loss: 1.5793\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0555 - val_loss: 1.5774\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0500 - val_loss: 1.5758\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0490 - val_loss: 1.5778\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0484 - val_loss: 1.5836\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0448 - val_loss: 1.5865\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0489 - val_loss: 1.5787\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0546 - val_loss: 1.5885\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0520 - val_loss: 1.5873\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0459 - val_loss: 1.5704\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0495 - val_loss: 1.5887\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0511 - val_loss: 1.5845\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0471 - val_loss: 1.5824\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0484 - val_loss: 1.5805\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0512 - val_loss: 1.5828\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0462 - val_loss: 1.5799\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0525 - val_loss: 1.5708\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0578 - val_loss: 1.5930\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0508 - val_loss: 1.5916\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0627 - val_loss: 1.5759\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0580 - val_loss: 1.5938\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0653 - val_loss: 1.5632\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0671 - val_loss: 1.5806\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0622 - val_loss: 1.5844\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0692 - val_loss: 1.5925\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0711 - val_loss: 1.5952\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0808 - val_loss: 1.5894\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0764 - val_loss: 1.5851\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0753 - val_loss: 1.5880\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0615 - val_loss: 1.5946\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0528 - val_loss: 1.5785\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0494 - val_loss: 1.5811\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0437 - val_loss: 1.5768\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0414 - val_loss: 1.6042\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0429 - val_loss: 1.5801\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0419 - val_loss: 1.5873\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.0461 - val_loss: 1.5856\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0491 - val_loss: 1.5771\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0568 - val_loss: 1.5836\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0506 - val_loss: 1.5808\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0442 - val_loss: 1.5789\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0515 - val_loss: 1.5797\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0702 - val_loss: 1.5725\n",
      "Epoch 237/300\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0534 - val_loss: 1.5977\n",
      "Epoch 238/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0434 - val_loss: 1.5871\n",
      "Epoch 239/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0473 - val_loss: 1.5838\n",
      "Epoch 240/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0517 - val_loss: 1.5734\n",
      "Epoch 241/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0530 - val_loss: 1.5954\n",
      "Epoch 242/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0518 - val_loss: 1.5798\n",
      "Epoch 243/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0504 - val_loss: 1.5814\n",
      "Epoch 244/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0509 - val_loss: 1.5776\n",
      "Epoch 245/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0549 - val_loss: 1.5856\n",
      "Epoch 246/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0686 - val_loss: 1.5792\n",
      "Epoch 247/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0557 - val_loss: 1.6058\n",
      "Epoch 248/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0519 - val_loss: 1.5880\n",
      "Epoch 249/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0439 - val_loss: 1.5790\n",
      "Epoch 250/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0520 - val_loss: 1.5801\n",
      "Epoch 251/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0591 - val_loss: 1.5751\n",
      "Epoch 252/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0558 - val_loss: 1.5765\n",
      "Epoch 253/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0491 - val_loss: 1.5900\n",
      "Epoch 254/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0532 - val_loss: 1.5739\n",
      "Epoch 255/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0699 - val_loss: 1.5898\n",
      "Epoch 256/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0530 - val_loss: 1.5863\n",
      "Epoch 257/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0482 - val_loss: 1.5850\n",
      "Epoch 258/300\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0449 - val_loss: 1.5794\n",
      "Epoch 259/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0497 - val_loss: 1.5666\n",
      "Epoch 260/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0634 - val_loss: 1.5807\n",
      "Epoch 261/300\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0529 - val_loss: 1.5924\n",
      "Epoch 262/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0510 - val_loss: 1.5783\n",
      "Epoch 263/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0539 - val_loss: 1.5825\n",
      "Epoch 264/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0491 - val_loss: 1.5925\n",
      "Epoch 265/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0499 - val_loss: 1.5787\n",
      "Epoch 266/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0452 - val_loss: 1.5766\n",
      "Epoch 267/300\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0439 - val_loss: 1.5792\n",
      "Epoch 268/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0386 - val_loss: 1.5766\n",
      "Epoch 269/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0389 - val_loss: 1.5823\n",
      "Epoch 270/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0366 - val_loss: 1.5870\n",
      "Epoch 271/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0450 - val_loss: 1.5700\n",
      "Epoch 272/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0451 - val_loss: 1.5786\n",
      "Epoch 273/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0440 - val_loss: 1.5876\n",
      "Epoch 274/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0354 - val_loss: 1.5774\n",
      "Epoch 275/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0390 - val_loss: 1.5886\n",
      "Epoch 276/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0377 - val_loss: 1.5739\n",
      "Epoch 277/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0379 - val_loss: 1.5825\n",
      "Epoch 278/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0351 - val_loss: 1.5757\n",
      "Epoch 279/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0390 - val_loss: 1.5722\n",
      "Epoch 280/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0369 - val_loss: 1.5841\n",
      "Epoch 281/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0428 - val_loss: 1.5878\n",
      "Epoch 282/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0419 - val_loss: 1.5953\n",
      "Epoch 283/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0444 - val_loss: 1.5815\n",
      "Epoch 284/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0386 - val_loss: 1.5749\n",
      "Epoch 285/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0413 - val_loss: 1.5845\n",
      "Epoch 286/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0415 - val_loss: 1.5720\n",
      "Epoch 287/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.0386 - val_loss: 1.5725\n",
      "Epoch 288/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0410 - val_loss: 1.5703\n",
      "Epoch 289/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0460 - val_loss: 1.5845\n",
      "Epoch 290/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0454 - val_loss: 1.5787\n",
      "Epoch 291/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0495 - val_loss: 1.5733\n",
      "Epoch 292/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0469 - val_loss: 1.5743\n",
      "Epoch 293/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0418 - val_loss: 1.5785\n",
      "Epoch 294/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0458 - val_loss: 1.5764\n",
      "Epoch 295/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0551 - val_loss: 1.5703\n",
      "Epoch 296/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0448 - val_loss: 1.5848\n",
      "Epoch 297/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.0507 - val_loss: 1.5890\n",
      "Epoch 298/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0616 - val_loss: 1.5863\n",
      "Epoch 299/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.0457 - val_loss: 1.5720\n",
      "Epoch 300/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0505 - val_loss: 1.5816\n",
      "train 0.16137119017571183 0.6542154487460116 0.2466636801148998\n",
      "test 1.5816297795740486 1.4872528866796857 1.0634571926130605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c2e0eed408>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = i = Input(shape=(30, 1))\n",
    "x = BatchNormalization()(x)\n",
    "x = Convolution1D(filters=4, kernel_size=8, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=8, kernel_size=4, dilation_rate=2, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=16, kernel_size=2, dilation_rate=4, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=32, kernel_size=1, dilation_rate=8, padding='same', activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(30, activation='relu')(x)\n",
    "# x = Dense(30, activation='relu', kernel_regularizer='l2')(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "\n",
    "\n",
    "model = keras.Model(i, x)\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-3), loss='mae')\n",
    "hist = model.fit(train_data_gen, epochs=300, validation_data=test_data_gen)\n",
    "evaluate_model(model)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "8/8 [==============================] - 1s 136ms/step - loss: 1.1859 - val_loss: 1.9355\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 1.0538 - val_loss: 1.8313\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.9495 - val_loss: 1.7432\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.8707 - val_loss: 1.6808\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.8081 - val_loss: 1.6305\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7614 - val_loss: 1.5999\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7277 - val_loss: 1.5729\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.7013 - val_loss: 1.5546\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6797 - val_loss: 1.5438\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6669 - val_loss: 1.5323\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.6517 - val_loss: 1.5399\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.6468 - val_loss: 1.5180\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6444 - val_loss: 1.5388\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6404 - val_loss: 1.5496\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.6230 - val_loss: 1.5281\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.6181 - val_loss: 1.5230\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6182 - val_loss: 1.5349\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6130 - val_loss: 1.5470\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6066 - val_loss: 1.5727\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.6081 - val_loss: 1.5369\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.5933 - val_loss: 1.5258\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5833 - val_loss: 1.5120\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.5778 - val_loss: 1.5114\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5634 - val_loss: 1.5355\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.5587 - val_loss: 1.5225\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.5490 - val_loss: 1.5374\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.5392 - val_loss: 1.5607\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.5310 - val_loss: 1.5719\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.5347 - val_loss: 1.5517\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5377 - val_loss: 1.5668\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.5119 - val_loss: 1.5517\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.5019 - val_loss: 1.6075\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4924 - val_loss: 1.5753\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4742 - val_loss: 1.5903\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4592 - val_loss: 1.6032\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4605 - val_loss: 1.6147\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4591 - val_loss: 1.6143\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.4586 - val_loss: 1.6412\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4417 - val_loss: 1.6320\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.4249 - val_loss: 1.6789\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.4374 - val_loss: 1.6361\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4322 - val_loss: 1.6227\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.4672 - val_loss: 1.6352\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4429 - val_loss: 1.7056\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.4361 - val_loss: 1.6654\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.4392 - val_loss: 1.7014\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.4371 - val_loss: 1.6503\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.4138 - val_loss: 1.7102\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.4823 - val_loss: 1.7160\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.4568 - val_loss: 1.7087\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4245 - val_loss: 1.6724\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.4273 - val_loss: 1.6474\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3913 - val_loss: 1.6744\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3804 - val_loss: 1.6869\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.3683 - val_loss: 1.7094\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.3591 - val_loss: 1.6995\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3764 - val_loss: 1.6616\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.4169 - val_loss: 1.7010\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.3834 - val_loss: 1.6966\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3483 - val_loss: 1.6935\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3521 - val_loss: 1.7168\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3385 - val_loss: 1.6871\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3279 - val_loss: 1.7264\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.3349 - val_loss: 1.7219\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.3485 - val_loss: 1.6953\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3111 - val_loss: 1.7372\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.3729 - val_loss: 1.6820\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.3285 - val_loss: 1.7537\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3135 - val_loss: 1.7256\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.3437 - val_loss: 1.7486\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3225 - val_loss: 1.7182\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2991 - val_loss: 1.7290\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3051 - val_loss: 1.7651\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.3116 - val_loss: 1.7416\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2985 - val_loss: 1.7755\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.2879 - val_loss: 1.7924\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3424 - val_loss: 1.7463\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.2947 - val_loss: 1.7533\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2929 - val_loss: 1.7615\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2938 - val_loss: 1.7490\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2693 - val_loss: 1.7657\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.3351 - val_loss: 1.7600\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.3134 - val_loss: 1.7665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2790 - val_loss: 1.7507\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3105 - val_loss: 1.7557\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3485 - val_loss: 1.7159\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3266 - val_loss: 1.7556\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.3314 - val_loss: 1.7364\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2948 - val_loss: 1.7456\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2607 - val_loss: 1.8094\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.2518 - val_loss: 1.7377\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2533 - val_loss: 1.7653\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.2674 - val_loss: 1.7807\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2476 - val_loss: 1.7549\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2481 - val_loss: 1.7704\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2568 - val_loss: 1.7611\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.2464 - val_loss: 1.7628\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2367 - val_loss: 1.7687\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2490 - val_loss: 1.7476\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2451 - val_loss: 1.7904\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2423 - val_loss: 1.7593\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2388 - val_loss: 1.7760\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2291 - val_loss: 1.7741\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2187 - val_loss: 1.7382\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.2209 - val_loss: 1.8200\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2482 - val_loss: 1.7873\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.2370 - val_loss: 1.7719\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2481 - val_loss: 1.7520\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2453 - val_loss: 1.7858\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2376 - val_loss: 1.7582\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.2584 - val_loss: 1.7659\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2241 - val_loss: 1.7737\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2671 - val_loss: 1.7549\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2639 - val_loss: 1.7898\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3158 - val_loss: 1.7874\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3603 - val_loss: 1.7645\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.3015 - val_loss: 1.7473\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2346 - val_loss: 1.7806\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2397 - val_loss: 1.7792\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2272 - val_loss: 1.7547\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2082 - val_loss: 1.8027\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2231 - val_loss: 1.8153\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2738 - val_loss: 1.7495\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2628 - val_loss: 1.8194\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2921 - val_loss: 1.7614\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.2680 - val_loss: 1.8028\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2772 - val_loss: 1.7855\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.2396 - val_loss: 1.7793\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.2492 - val_loss: 1.7907\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2392 - val_loss: 1.8068\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.2798 - val_loss: 1.7384\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.3326 - val_loss: 1.7719\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2580 - val_loss: 1.7637\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2437 - val_loss: 1.7912\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2716 - val_loss: 1.7725\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.2969 - val_loss: 1.7929\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.3345 - val_loss: 1.7707\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.2943 - val_loss: 1.7549\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2552 - val_loss: 1.7856\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2359 - val_loss: 1.7642\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2485 - val_loss: 1.8038\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2310 - val_loss: 1.7743\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2232 - val_loss: 1.7582\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1996 - val_loss: 1.7659\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1970 - val_loss: 1.8115\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1985 - val_loss: 1.7771\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1894 - val_loss: 1.8005\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1804 - val_loss: 1.7912\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.1810 - val_loss: 1.8102\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.1853 - val_loss: 1.8035\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2230 - val_loss: 1.7778\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2032 - val_loss: 1.8013\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.2192 - val_loss: 1.7754\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2310 - val_loss: 1.8177\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2539 - val_loss: 1.8510\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2925 - val_loss: 1.7564\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.3079 - val_loss: 1.7924\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 0s 53ms/step - loss: 0.2456 - val_loss: 1.8042\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2327 - val_loss: 1.7637\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2740 - val_loss: 1.7661\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.2309 - val_loss: 1.7987\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2065 - val_loss: 1.7797\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2353 - val_loss: 1.7654\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.2868 - val_loss: 1.7710\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2836 - val_loss: 1.7901\n",
      "Epoch 166/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2465 - val_loss: 1.7978\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.2246 - val_loss: 1.7954\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1957 - val_loss: 1.7888\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2302 - val_loss: 1.7717\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2337 - val_loss: 1.7803\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2043 - val_loss: 1.7717\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1983 - val_loss: 1.7881\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2097 - val_loss: 1.7783\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1884 - val_loss: 1.8175\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1884 - val_loss: 1.7801\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1845 - val_loss: 1.8099\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.1764 - val_loss: 1.8123\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.1765 - val_loss: 1.8154\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.1686 - val_loss: 1.7951\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1717 - val_loss: 1.8199\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1744 - val_loss: 1.7860\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1733 - val_loss: 1.8048\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.1863 - val_loss: 1.8035\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1937 - val_loss: 1.7904\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1722 - val_loss: 1.8213\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.1754 - val_loss: 1.7749\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.2072 - val_loss: 1.7853\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2103 - val_loss: 1.7473\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2181 - val_loss: 1.7891\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2087 - val_loss: 1.7752\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.2309 - val_loss: 1.8076\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.2168 - val_loss: 1.7780\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2229 - val_loss: 1.8049\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.2513 - val_loss: 1.7515\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2592 - val_loss: 1.7663\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2305 - val_loss: 1.7999\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.2467 - val_loss: 1.7928\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.2193 - val_loss: 1.7723\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1747 - val_loss: 1.7652\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.2168 - val_loss: 1.7764\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1959 - val_loss: 1.7831\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1806 - val_loss: 1.8275\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.1898 - val_loss: 1.7940\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1712 - val_loss: 1.7924\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.1624 - val_loss: 1.7984\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1566 - val_loss: 1.7741\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1522 - val_loss: 1.7829\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.1564 - val_loss: 1.7671\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1540 - val_loss: 1.7866\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.1601 - val_loss: 1.8035\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1533 - val_loss: 1.7968\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1478 - val_loss: 1.8125\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1483 - val_loss: 1.7809\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.1460 - val_loss: 1.7917\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1588 - val_loss: 1.7656\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1569 - val_loss: 1.7842\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1554 - val_loss: 1.7820\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1651 - val_loss: 1.7868\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1485 - val_loss: 1.8070\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1610 - val_loss: 1.7827\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.1539 - val_loss: 1.7923\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1458 - val_loss: 1.7720\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.1535 - val_loss: 1.7964\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.1915 - val_loss: 1.7844\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.1628 - val_loss: 1.8028\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1538 - val_loss: 1.7966\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1688 - val_loss: 1.7993\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1564 - val_loss: 1.7785\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1630 - val_loss: 1.7731\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1949 - val_loss: 1.7623\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.1822 - val_loss: 1.7797\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.1592 - val_loss: 1.7844\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1544 - val_loss: 1.7742\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1516 - val_loss: 1.7977\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.1513 - val_loss: 1.7857\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.1516 - val_loss: 1.7823\n",
      "Epoch 237/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = i = Input(shape=(30, 1))\n",
    "x = BatchNormalization()(x)\n",
    "x = Convolution1D(filters=4, kernel_size=8, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=8, kernel_size=4, dilation_rate=2, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=16, kernel_size=2, dilation_rate=4, padding='same', activation='relu')(x)\n",
    "x = Convolution1D(filters=32, kernel_size=1, dilation_rate=8, padding='same', activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "# x = Dense(30, activation='relu')(x)\n",
    "x = Dense(30, activation='relu', kernel_regularizer='l2')(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "\n",
    "\n",
    "model = keras.Model(i, x)\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-3), loss='mae')\n",
    "hist = model.fit(train_data_gen, epochs=300, validation_data=test_data_gen)\n",
    "evaluate_model(model)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1c2b472f2c8>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.squeeze(model.predict(data_gen)), data_gen_df[30:])\n",
    "# spx.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.388946</td>\n",
       "      <td>-0.003588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.633396</td>\n",
       "      <td>0.008036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.675272</td>\n",
       "      <td>0.008818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008813</td>\n",
       "      <td>0.000725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.059887</td>\n",
       "      <td>0.001275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>0.049352</td>\n",
       "      <td>0.002710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>0.194991</td>\n",
       "      <td>0.002303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>0.026615</td>\n",
       "      <td>-0.004404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>0.187227</td>\n",
       "      <td>0.003159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>0.214284</td>\n",
       "      <td>0.003441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1229 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "0    -0.388946 -0.003588\n",
       "1     0.633396  0.008036\n",
       "2     0.675272  0.008818\n",
       "3     0.008813  0.000725\n",
       "4     0.059887  0.001275\n",
       "...        ...       ...\n",
       "1224  0.049352  0.002710\n",
       "1225  0.194991  0.002303\n",
       "1226  0.026615 -0.004404\n",
       "1227  0.187227  0.003159\n",
       "1228  0.214284  0.003441\n",
       "\n",
       "[1229 rows x 2 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.vstack([np.squeeze(model.predict(data_gen)), spx[30:]])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " LogicalDevice(name='/device:GPU:0', device_type='GPU'),\n",
       " LogicalDevice(name='/device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
