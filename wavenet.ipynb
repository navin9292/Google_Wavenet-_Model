{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "yf.pdr_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  9 of 9 completed\n",
      "\n",
      "--- Full dataset\n",
      "\n",
      "--- Full dataset returns\n",
      "\n",
      "--- Train set unadjusted\n",
      "\n",
      "--- Test set unadjusted\n",
      "\n",
      "--- mu train:\n",
      "\n",
      "--- sigma train:\n",
      "\n",
      "--- Train set adjusted\n",
      "\n",
      "--- Test set adjusted\n"
     ]
    }
   ],
   "source": [
    "# Tickers\n",
    "tickers = ['^GSPC', '^VIX', '^TNX', 'EURUSD=X', 'EURJPY=X', 'GBPJPY=X', 'EURGBP=X', 'GBPUSD=X', 'NFLX']\n",
    "\n",
    "# dates\n",
    "startdate = '2015-1-1'\n",
    "train_end = '2017-12-31'\n",
    "test_start = '2018-01-01'\n",
    "enddate = '2018-12-31'\n",
    "\n",
    "# data Close\n",
    "data_close = pdr.get_data_yahoo(tickers, start=startdate, end=enddate)['Adj Close']\n",
    "print('\\n--- Full dataset')\n",
    "# display(data_close)\n",
    "\n",
    "# data returns\n",
    "data = data_close.shift(1) / data_close - 1\n",
    "print('\\n--- Full dataset returns')\n",
    "# display(data)\n",
    "\n",
    "# Sets\n",
    "data_train = data[startdate:train_end].dropna()\n",
    "data_test = data[test_start:enddate].dropna()\n",
    "print('\\n--- Train set unadjusted')\n",
    "# display(data_train)\n",
    "print('\\n--- Test set unadjusted')\n",
    "# display(data_test)\n",
    "\n",
    "# mu sigma train\n",
    "mu_train = data_train.mean()\n",
    "sigma_train = data_train.std()\n",
    "print('\\n--- mu train:')\n",
    "# display(pd.DataFrame(mu_train, columns=['Train set Mean']))\n",
    "print('\\n--- sigma train:')\n",
    "# display(pd.DataFrame(sigma_train, columns=['Train set st dev']))\n",
    "\n",
    "# Final sets\n",
    "data_train_prod = (data_train - mu_train) / sigma_train\n",
    "data_test_prod = (data_test - mu_train) / sigma_train\n",
    "print('\\n--- Train set adjusted')\n",
    "# display(data_train_prod)\n",
    "print('\\n--- Test set adjusted')\n",
    "# display(data_test_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.graphics.tsaplots import plot_acf\n",
    "# plot_acf(data_train_prod['^GSPC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['EURGBP=X', 'EURJPY=X', 'EURUSD=X', 'GBPJPY=X', 'GBPUSD=X', 'NFLX',\n",
       "       '^GSPC', '^TNX', '^VIX'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_prod.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.preprocessing.sequence.TimeseriesGenerator at 0x18ffbe92688>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_length = 2 ** (4-1) * 2 * 4\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "train_gen_df = data_train_prod['^GSPC']\n",
    "test_gen_df = data_test_prod['^GSPC']\n",
    "train_data_gen = TimeseriesGenerator(train_gen_df.values, train_gen_df, length=win_length)\n",
    "test_data_gen = TimeseriesGenerator(test_gen_df.values, test_gen_df, length=win_length)\n",
    "train_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionedTimeseriesGenerator(TimeseriesGenerator):\n",
    "    def __init__(self, input_timeseries, condition_timeseries):\n",
    "        self.input_timeseres = input_timeseries\n",
    "        self.condition_timeseries = condition_timeseries\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_timeseres)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        samples, targets = self.input_timeseres[index]\n",
    "        samples_cond, _ = self.condition_timeseries[index]\n",
    "        return (samples, samples_cond), targets\n",
    "\n",
    "    def get_config(self):\n",
    "        d = {}\n",
    "        d.update(self.input_timeseres.get_config())\n",
    "        d.update(self.condition_timeseries.get_config())\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_condition_gen_df = data_train_prod['^TNX']\n",
    "train_data_condition_gen = TimeseriesGenerator(train_condition_gen_df.values, train_condition_gen_df, length=win_length)\n",
    "cond_gen = ConditionedTimeseriesGenerator(train_data_gen, train_data_condition_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_condition_gen_df = data_test_prod['^TNX']\n",
    "test_data_condition_gen = TimeseriesGenerator(test_condition_gen_df.values, test_condition_gen_df, length=win_length)\n",
    "test_cond_gen = ConditionedTimeseriesGenerator(test_data_gen, test_data_condition_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/usernaamee/keras-wavenet/blob/master/simple-generative-model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Convolution1D, Convolution2D, Flatten, Dense, \\\n",
    "    Input, Lambda, Activation, BatchNormalization, Dropout\n",
    "from tensorflow import keras\n",
    "import tensorflow\n",
    "\n",
    "def WaveNetBlock(filters, kernel_size, dilation_rate, activation=None, connection_parametrized=False):\n",
    "    def f(input_):\n",
    "        residual = input_\n",
    "        tanh_out = Convolution1D(filters, kernel_size,\n",
    "                                       dilation_rate=dilation_rate,\n",
    "                                       padding='same',\n",
    "                                       activation='tanh')(input_)\n",
    "        sigmoid_out = Convolution1D(filters, kernel_size,\n",
    "                                          dilation_rate=dilation_rate,\n",
    "                                          padding='same',\n",
    "                                          activation='relu')(input_)\n",
    "        merged = keras.layers.Multiply()([tanh_out, sigmoid_out])\n",
    "        skip_out = Convolution1D(1, 1, activation=activation, padding='same')(merged)\n",
    "        if connection_parametrized:\n",
    "            residual = Dense(1, activation='linear', use_bias=False)(residual)\n",
    "        out = keras.layers.Add()([skip_out, residual])\n",
    "        return out, skip_out\n",
    "#         return skip_out\n",
    "    return f\n",
    "\n",
    "\n",
    "# def WaveNetBlock(filters, kernel_size, dilation_rate, activation=None):\n",
    "#     def f(input_):\n",
    "#         residual = input_\n",
    "#         conv = Convolution1D(filters, kernel_size,\n",
    "#                                        dilation_rate=dilation_rate,\n",
    "#                                        padding='same',\n",
    "#                                        activation='relu')(input_)\n",
    "#         if residual.shape[-1] != filters:\n",
    "#             residual = Convolution1D(filters, 1, activation=activation, padding='same')(residual)\n",
    "#         out = keras.layers.Add()([conv, residual])\n",
    "#         out = Activation('relu')(out)\n",
    "#         return out, conv\n",
    "#     return f\n",
    "\n",
    "def ConditionedBlock(filters, kernel_size, dilation_rate, activation=None):\n",
    "    def f(input_, condition_input):\n",
    "        input_wave, _ = WaveNetBlock(filters, kernel_size, dilation_rate, activation=activation, connection_parametrized=True)(input_)\n",
    "        condition_wave, _ = WaveNetBlock(filters, kernel_size, dilation_rate, activation=activation, connection_parametrized=True)(condition_input)   \n",
    "        merged = keras.layers.Add()([input_wave, condition_wave])\n",
    "        out = Convolution1D(1, 1, activation=activation, padding='same')(merged)\n",
    "        return out\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, hist):\n",
    "    total_mae = mean_absolute_error(train_gen_df[win_length:], np.zeros_like(train_gen_df[win_length:]))\n",
    "    model_mae = mean_absolute_error(np.squeeze(model.predict(train_data_gen)), train_gen_df[win_length:]) \n",
    "    print(f'train MAE: {model_mae: 0.4f} ({model_mae/total_mae: 0.2%}), total={total_mae: 0.4f}')\n",
    "    total_mae = mean_absolute_error(test_gen_df[win_length:], np.zeros_like(test_gen_df[win_length:]))\n",
    "    model_mae = mean_absolute_error(np.squeeze(model.predict(test_data_gen)), test_gen_df[win_length:]) \n",
    "    print(f'test MAE: {model_mae: 0.4f} ({model_mae/total_mae: 0.2%}), total={total_mae: 0.4f}')\n",
    "    plt.scatter(np.squeeze(model.predict(train_data_gen)), train_gen_df[win_length:])\n",
    "    plt.title('training')\n",
    "    plt.xlabel('prediction')\n",
    "    plt.subplots()\n",
    "    plt.scatter(np.squeeze(model.predict(test_data_gen)), test_gen_df[win_length:])\n",
    "    plt.title('testing')\n",
    "    plt.xlabel('prediction')\n",
    "    pd.DataFrame(hist.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Dilated Covnet, filter=1, kernel=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_one():\n",
    "    x = i = Input(shape=(win_length, 1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution1D(filters=1, kernel_size=2, dilation_rate=2**0, padding='same', activation='relu')(x)\n",
    "    x = Convolution1D(filters=1, kernel_size=2, dilation_rate=2**1, padding='same', activation='relu')(x)\n",
    "    x = Convolution1D(filters=1, kernel_size=2, dilation_rate=2**2, padding='same', activation='relu')(x)\n",
    "    x = Convolution1D(filters=1, kernel_size=2, dilation_rate=2**3, padding='same', activation='relu')(x)\n",
    "    \n",
    "    x = Convolution1D(filters=1, kernel_size=1, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='linear', kernel_regularizer='l2')(x)\n",
    "    model = keras.Model(i, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_one():\n",
    "    x = i = Input(shape=(win_length, 1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x, _ = WaveNetBlock(filters=1, kernel_size=2, dilation_rate=2**0, activation='relu')(x)\n",
    "    x, _ = WaveNetBlock(filters=1, kernel_size=2, dilation_rate=2**1, activation='relu')(x)\n",
    "    x, _ = WaveNetBlock(filters=1, kernel_size=2, dilation_rate=2**2, activation='relu')(x)\n",
    "    x, _ = WaveNetBlock(filters=1, kernel_size=2, dilation_rate=2**3, activation='relu')(x)\n",
    "\n",
    "    x = Convolution1D(filters=1, kernel_size=1, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='linear', kernel_regularizer='l2')(x)\n",
    "    model = keras.Model(i, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_multi_extra_layer():\n",
    "    x = i = Input(shape=(win_length, 1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution1D(filters=4,  kernel_size=2, dilation_rate=2**0, padding='same', activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution1D(filters=8,  kernel_size=2, dilation_rate=2**1, padding='same', activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution1D(filters=16, kernel_size=2, dilation_rate=2**2, padding='same', activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution1D(filters=32, kernel_size=2, dilation_rate=2**3, padding='same', activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    x = Convolution1D(filters=1, kernel_size=1, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(win_length, activation='relu', kernel_regularizer='l2')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation='linear', kernel_regularizer='l2')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    model = keras.Model(i, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_multi_extra_layer():\n",
    "    x = i = Input(shape=(win_length, 1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x, _ = WaveNetBlock(filters=3, kernel_size=2, dilation_rate=2**0, activation='relu')(x)\n",
    "    x, _ = WaveNetBlock(filters=3, kernel_size=2, dilation_rate=2**1, activation='relu')(x)\n",
    "    x, _ = WaveNetBlock(filters=3, kernel_size=2, dilation_rate=2**2, activation='relu')(x)\n",
    "    x, _ = WaveNetBlock(filters=3, kernel_size=2, dilation_rate=2**3, activation='relu')(x)\n",
    "\n",
    "    x = Convolution1D(filters=1, kernel_size=1, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(i, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_multi_extra_layer_conditional():\n",
    "    x = i = Input(shape=(win_length, 1))\n",
    "    x = BatchNormalization()(x)\n",
    "    c = c0 = Input(shape=(win_length, 1))\n",
    "    c = BatchNormalization()(c)\n",
    "    \n",
    "    x = ConditionedBlock(filters=3, kernel_size=2, dilation_rate=2**0, activation='relu')(x, c)\n",
    "    \n",
    "    x, _ = WaveNetBlock(filters=3, kernel_size=2, dilation_rate=2**0, activation='relu')(x)\n",
    "    x, _ = WaveNetBlock(filters=3, kernel_size=2, dilation_rate=2**1, activation='relu')(x)\n",
    "    x, _ = WaveNetBlock(filters=3, kernel_size=2, dilation_rate=2**2, activation='relu')(x)\n",
    "    x, _ = WaveNetBlock(filters=3, kernel_size=2, dilation_rate=2**3, activation='relu')(x)\n",
    "\n",
    "    x = Convolution1D(filters=1, kernel_size=1, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    model = keras.Model([i, c0], x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_tensorboard_callback = lambda key: keras.callbacks.TensorBoard(log_dir=f\"logs/fit/{key}/\" + pd.to_datetime('now').strftime(\"%Y%m%d-%H%M%S\"), histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = conv_one()\n",
    "# model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-3), loss='mae')\n",
    "# hist = model.fit(train_data_gen, epochs=100, validation_data=test_data_gen, verbose=0, callbacks=[make_tensorboard_callback('conv_one')])\n",
    "# evaluate_model(model, hist)\n",
    "# hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wavenet, filter=1, kernel=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = wave_one()\n",
    "model1.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-3), loss='mae')\n",
    "hist = model1.fit(train_data_gen, epochs=500, validation_data=test_data_gen, verbose=0, callbacks=[make_tensorboard_callback('wave_one')])\n",
    "evaluate_model(model1, hist)\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet, variable filters / kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = conv_multi_extra_layer()\n",
    "# model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-3), loss='mae')\n",
    "# hist = model.fit(train_data_gen, epochs=100, validation_data=test_data_gen, verbose=0, callbacks=[make_tensorboard_callback('conv_multi_extra_layer')])\n",
    "# evaluate_model(model, hist)\n",
    "# hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WaveNet, variable filter/kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wave_multi_extra_layer()\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-3), loss='mae')\n",
    "hist = model.fit(train_data_gen, epochs=500, validation_data=test_data_gen, verbose=0, callbacks=[make_tensorboard_callback('wave_multi_extra_layer')])\n",
    "evaluate_model(model, hist)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/6 [=========>....................] - ETA: 1s - loss: 0.6981WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.332003). Check your callbacks.\n",
      "6/6 [==============================] - 3s 582ms/step - loss: 0.7633 - val_loss: 0.8532\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 1s 181ms/step - loss: 0.6872 - val_loss: 0.8217\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 0.6707 - val_loss: 0.8104\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 0.6624 - val_loss: 0.8087\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.6602 - val_loss: 0.8094\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.6596 - val_loss: 0.8098\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6595 - val_loss: 0.8100\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.6595 - val_loss: 0.8102\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6593 - val_loss: 0.8103\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.6590 - val_loss: 0.8103\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.6584 - val_loss: 0.8105\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.6584 - val_loss: 0.8109\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 0.6580 - val_loss: 0.8119\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.6572 - val_loss: 0.8120\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.6565 - val_loss: 0.8145\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.6562 - val_loss: 0.8180\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.6546 - val_loss: 0.8197\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 0.6533 - val_loss: 0.8238\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.6521 - val_loss: 0.8289\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.6508 - val_loss: 0.8323\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6494 - val_loss: 0.8392\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.6481 - val_loss: 0.8449\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6454 - val_loss: 0.8449\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6424 - val_loss: 0.8495\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6406 - val_loss: 0.8521\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 0.6397 - val_loss: 0.8561\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 0.6379 - val_loss: 0.8497\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.6366 - val_loss: 0.8613\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6352 - val_loss: 0.8512\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.6341 - val_loss: 0.8650\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 0.6337 - val_loss: 0.8608\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.6321 - val_loss: 0.8568\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6303 - val_loss: 0.8716\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.6328 - val_loss: 0.8517\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6344 - val_loss: 0.8452\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6325 - val_loss: 0.8556\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6302 - val_loss: 0.8689\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6317 - val_loss: 0.8527\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6320 - val_loss: 0.8469\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6297 - val_loss: 0.8571\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.6337 - val_loss: 0.8840\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6362 - val_loss: 0.8649\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6324 - val_loss: 0.8506\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6292 - val_loss: 0.8579\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6276 - val_loss: 0.8594\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6280 - val_loss: 0.8642\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6268 - val_loss: 0.8588\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6260 - val_loss: 0.8529\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6275 - val_loss: 0.8555\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.6265 - val_loss: 0.8629\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6258 - val_loss: 0.8560\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6266 - val_loss: 0.8638\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6259 - val_loss: 0.8611\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6251 - val_loss: 0.8627\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6246 - val_loss: 0.8599\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6246 - val_loss: 0.8756\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6259 - val_loss: 0.8579\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6254 - val_loss: 0.8565\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6247 - val_loss: 0.8779\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6288 - val_loss: 0.8765\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6279 - val_loss: 0.8436\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6357 - val_loss: 0.8425\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6281 - val_loss: 0.8661\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6267 - val_loss: 0.8698\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6237 - val_loss: 0.8608\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6248 - val_loss: 0.8686\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6249 - val_loss: 0.8581\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.6251 - val_loss: 0.8597\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.6239 - val_loss: 0.8639\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6235 - val_loss: 0.8580\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.6236 - val_loss: 0.8697\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.6243 - val_loss: 0.8677\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6245 - val_loss: 0.8587\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.6236 - val_loss: 0.8713\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.6245 - val_loss: 0.8611\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.6247 - val_loss: 0.8611\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.6244 - val_loss: 0.8659\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6225 - val_loss: 0.8614\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.6240 - val_loss: 0.8634\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 38ms/step - loss: 0.6229 - val_loss: 0.8693\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.6259 - val_loss: 0.8592\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.6246 - val_loss: 0.8596\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6255 - val_loss: 0.8796\n",
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 40ms/step - loss: 0.6277 - val_loss: 0.8685\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.6294 - val_loss: 0.8467\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 1s 170ms/step - loss: 0.6345 - val_loss: 0.8484\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 0.6274 - val_loss: 0.8635\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 1s 123ms/step - loss: 0.6235 - val_loss: 0.8730\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.6254 - val_loss: 0.8672\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6237 - val_loss: 0.8706\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.6244 - val_loss: 0.8568\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6243 - val_loss: 0.8564\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6232 - val_loss: 0.8674\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6248 - val_loss: 0.8623\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6225 - val_loss: 0.8580\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.6228 - val_loss: 0.8647\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6223 - val_loss: 0.8585\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6223 - val_loss: 0.8594\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6258 - val_loss: 0.8823\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6263 - val_loss: 0.8652\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6240 - val_loss: 0.8544\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6237 - val_loss: 0.8593\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6225 - val_loss: 0.8634\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.6221 - val_loss: 0.8639\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6224 - val_loss: 0.8578\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6218 - val_loss: 0.8634\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.6225 - val_loss: 0.8635\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.6247 - val_loss: 0.8528\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.624 - 0s 46ms/step - loss: 0.6240 - val_loss: 0.8593\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 0.6214 - val_loss: 0.8658\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.6209 - val_loss: 0.8596\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 0.6225 - val_loss: 0.8608\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.6217 - val_loss: 0.8646\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6207 - val_loss: 0.8594\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.6222 - val_loss: 0.8594\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.6212 - val_loss: 0.8623\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6199 - val_loss: 0.8649\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6214 - val_loss: 0.8660\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.6212 - val_loss: 0.8514\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6235 - val_loss: 0.8490\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6227 - val_loss: 0.8598\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6216 - val_loss: 0.8615\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6210 - val_loss: 0.8559\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6225 - val_loss: 0.8555\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.6216 - val_loss: 0.8549\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6207 - val_loss: 0.8615\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6211 - val_loss: 0.8608\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6211 - val_loss: 0.8616\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.6209 - val_loss: 0.8641\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6200 - val_loss: 0.8530\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6211 - val_loss: 0.8581\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6198 - val_loss: 0.8684\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.6195 - val_loss: 0.8568\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.6206 - val_loss: 0.8637\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6198 - val_loss: 0.8626\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6199 - val_loss: 0.8597\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6197 - val_loss: 0.8588\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6192 - val_loss: 0.8673\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6199 - val_loss: 0.8611\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.6196 - val_loss: 0.8624\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6227 - val_loss: 0.8759\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6237 - val_loss: 0.8498\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6243 - val_loss: 0.8483\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6208 - val_loss: 0.8566\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6194 - val_loss: 0.8554\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6202 - val_loss: 0.8595\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6190 - val_loss: 0.8691\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6187 - val_loss: 0.8539\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.6193 - val_loss: 0.8603\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.6193 - val_loss: 0.8598\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6185 - val_loss: 0.8610\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 1s 192ms/step - loss: 0.6187 - val_loss: 0.8679\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 1s 128ms/step - loss: 0.6198 - val_loss: 0.8482\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 0.6211 - val_loss: 0.8564\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 0.6195 - val_loss: 0.8670\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.6203 - val_loss: 0.8706\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6189 - val_loss: 0.8700\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6209 - val_loss: 0.8595\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6195 - val_loss: 0.8558\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6190 - val_loss: 0.8760\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6192 - val_loss: 0.8593\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6200 - val_loss: 0.8503\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6213 - val_loss: 0.8549\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 0.6189 - val_loss: 0.8577\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6190 - val_loss: 0.8519\n",
      "Epoch 166/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6187 - val_loss: 0.8562\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6190 - val_loss: 0.8580\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6178 - val_loss: 0.8551\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6177 - val_loss: 0.8626\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6173 - val_loss: 0.8549\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.6176 - val_loss: 0.8619\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6182 - val_loss: 0.8473\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6182 - val_loss: 0.8505\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6166 - val_loss: 0.8633\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.6167 - val_loss: 0.8530\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6174 - val_loss: 0.8643\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.6169 - val_loss: 0.8584\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.6172 - val_loss: 0.8543\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6162 - val_loss: 0.8606\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6162 - val_loss: 0.8578\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.6175 - val_loss: 0.8553\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6165 - val_loss: 0.8551\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6171 - val_loss: 0.8613\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.6190 - val_loss: 0.8496\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6166 - val_loss: 0.8557\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.6163 - val_loss: 0.8541\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.6157 - val_loss: 0.8557\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 1s 173ms/step - loss: 0.6158 - val_loss: 0.8524\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.6154 - val_loss: 0.8580\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 0.6154 - val_loss: 0.8571\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.6154 - val_loss: 0.8605\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.6171 - val_loss: 0.8538\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6164 - val_loss: 0.8520\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6158 - val_loss: 0.8679\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6164 - val_loss: 0.8557\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6191 - val_loss: 0.8523\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6165 - val_loss: 0.8776\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.6197 - val_loss: 0.8596\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 0.6166 - val_loss: 0.8509\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.6162 - val_loss: 0.8613\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.6159 - val_loss: 0.8510\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 0.6156 - val_loss: 0.8605\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6150 - val_loss: 0.8504\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6159 - val_loss: 0.8584\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6151 - val_loss: 0.8588\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.6146 - val_loss: 0.8601\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.6140 - val_loss: 0.8471\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6158 - val_loss: 0.8507\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.6168 - val_loss: 0.8661\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6158 - val_loss: 0.8619\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6173 - val_loss: 0.8570\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6163 - val_loss: 0.8532\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6157 - val_loss: 0.8460\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6155 - val_loss: 0.8613\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6187 - val_loss: 0.8618\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6163 - val_loss: 0.8378\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6164 - val_loss: 0.8406\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6162 - val_loss: 0.8421\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6142 - val_loss: 0.8526\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6151 - val_loss: 0.8579\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6159 - val_loss: 0.8542\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6157 - val_loss: 0.8478\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6169 - val_loss: 0.8503\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6160 - val_loss: 0.8557\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.6166 - val_loss: 0.8553\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.6164 - val_loss: 0.8548\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6156 - val_loss: 0.8553\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6174 - val_loss: 0.8456\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6156 - val_loss: 0.8477\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.6186 - val_loss: 0.8518\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6172 - val_loss: 0.8537\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6174 - val_loss: 0.8583\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6152 - val_loss: 0.8409\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.6151 - val_loss: 0.8391\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6165 - val_loss: 0.8512\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 0.6151 - val_loss: 0.8511\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 0.6157 - val_loss: 0.8522\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 0.6142 - val_loss: 0.8497\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 0.6145 - val_loss: 0.8548\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.6157 - val_loss: 0.8445\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6157 - val_loss: 0.8507\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.6157 - val_loss: 0.8544\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.6143 - val_loss: 0.8505\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.6143 - val_loss: 0.8472\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6144 - val_loss: 0.8485\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6182 - val_loss: 0.8640\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6145 - val_loss: 0.8477\n",
      "Epoch 248/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6141 - val_loss: 0.8494\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6141 - val_loss: 0.8478\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6139 - val_loss: 0.8502\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.6160 - val_loss: 0.8470\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6154 - val_loss: 0.8438\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6135 - val_loss: 0.8459\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6140 - val_loss: 0.8512\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6166 - val_loss: 0.8508\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6156 - val_loss: 0.8475\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.6148 - val_loss: 0.8446\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6135 - val_loss: 0.8478\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6142 - val_loss: 0.8485\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 1s 170ms/step - loss: 0.6149 - val_loss: 0.8465\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.6152 - val_loss: 0.8527\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 0.6137 - val_loss: 0.8475\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.6140 - val_loss: 0.8471\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.6154 - val_loss: 0.8602\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.6133 - val_loss: 0.8502\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.6156 - val_loss: 0.8561\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6130 - val_loss: 0.8489\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6126 - val_loss: 0.8495\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6150 - val_loss: 0.8600\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6147 - val_loss: 0.8476\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6163 - val_loss: 0.8494\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6136 - val_loss: 0.8523\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6152 - val_loss: 0.8414\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6174 - val_loss: 0.8456\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6175 - val_loss: 0.8713\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6187 - val_loss: 0.8453\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6185 - val_loss: 0.8383\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.6161 - val_loss: 0.8554\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6173 - val_loss: 0.8482\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6153 - val_loss: 0.8376\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 1s 127ms/step - loss: 0.6156 - val_loss: 0.8463\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 0.6126 - val_loss: 0.8494\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.6122 - val_loss: 0.8499\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.6121 - val_loss: 0.8516\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.6122 - val_loss: 0.8564\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6115 - val_loss: 0.8476\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6113 - val_loss: 0.8532\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.6122 - val_loss: 0.8578\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.6129 - val_loss: 0.8577\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.6132 - val_loss: 0.8540\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.6140 - val_loss: 0.8503\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6129 - val_loss: 0.8538\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.6126 - val_loss: 0.8509\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.6114 - val_loss: 0.8474\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6126 - val_loss: 0.8534\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6158 - val_loss: 0.8588\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6136 - val_loss: 0.8411\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6170 - val_loss: 0.8435\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.6135 - val_loss: 0.8644\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.6124 - val_loss: 0.8409\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6136 - val_loss: 0.8391\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6124 - val_loss: 0.8551\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 0.6141 - val_loss: 0.8586\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.6138 - val_loss: 0.8453\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.6104 - val_loss: 0.8525\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 0.6137 - val_loss: 0.8463\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.6116 - val_loss: 0.8455\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6117 - val_loss: 0.8547\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6102 - val_loss: 0.8522\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6102 - val_loss: 0.8533\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.6109 - val_loss: 0.8527\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.6101 - val_loss: 0.8556\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6101 - val_loss: 0.8445\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6117 - val_loss: 0.8435\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6109 - val_loss: 0.8590\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6117 - val_loss: 0.8485\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.6134 - val_loss: 0.8462\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6107 - val_loss: 0.8564\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6107 - val_loss: 0.8441\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6107 - val_loss: 0.8456\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6094 - val_loss: 0.8540\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.6099 - val_loss: 0.8445\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6116 - val_loss: 0.8488\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.6130 - val_loss: 0.8532\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6109 - val_loss: 0.8440\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6120 - val_loss: 0.8498\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.6117 - val_loss: 0.8503\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 0.6113 - val_loss: 0.8500\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.6085 - val_loss: 0.8514\n",
      "Epoch 330/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.6093 - val_loss: 0.8532\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6090 - val_loss: 0.8575\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6073 - val_loss: 0.8561\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.6075 - val_loss: 0.8568\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.6068 - val_loss: 0.8600\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6084 - val_loss: 0.8538\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.6066 - val_loss: 0.8582\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6066 - val_loss: 0.8606\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6071 - val_loss: 0.8543\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6075 - val_loss: 0.8614\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6059 - val_loss: 0.8575\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6078 - val_loss: 0.8496\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6072 - val_loss: 0.8535\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6059 - val_loss: 0.8481\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6063 - val_loss: 0.8749\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.6074 - val_loss: 0.8694\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6050 - val_loss: 0.8570\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6076 - val_loss: 0.8625\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.6066 - val_loss: 0.8573\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6048 - val_loss: 0.8703\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6062 - val_loss: 0.8720\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6033 - val_loss: 0.8650\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.6023 - val_loss: 0.8618\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.6017 - val_loss: 0.8787\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.6042 - val_loss: 0.8572\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6039 - val_loss: 0.8758\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.6025 - val_loss: 0.8504\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.6065 - val_loss: 0.8466\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6077 - val_loss: 0.8599\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.6033 - val_loss: 0.8430\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.6049 - val_loss: 0.8560\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.6056 - val_loss: 0.8723\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6070 - val_loss: 0.8367\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.6172 - val_loss: 0.8323\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6099 - val_loss: 0.8635\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.6082 - val_loss: 0.8596\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.6019 - val_loss: 0.8498\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.6007 - val_loss: 0.8739\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6006 - val_loss: 0.8503\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.6019 - val_loss: 0.8614\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6014 - val_loss: 0.8622\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.5998 - val_loss: 0.8635\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5979 - val_loss: 0.8757\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.5992 - val_loss: 0.8553\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5992 - val_loss: 0.8615\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.5990 - val_loss: 0.8589\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.5978 - val_loss: 0.8664\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.5977 - val_loss: 0.8627\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.5989 - val_loss: 0.8630\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5978 - val_loss: 0.8599\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5964 - val_loss: 0.8603\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5951 - val_loss: 0.8638\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5970 - val_loss: 0.8641\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5956 - val_loss: 0.8676\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.5981 - val_loss: 0.8515\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5965 - val_loss: 0.8691\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5982 - val_loss: 0.8644\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5974 - val_loss: 0.8586\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.5958 - val_loss: 0.8766\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.5951 - val_loss: 0.8696\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 0.5948 - val_loss: 0.8762\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.5955 - val_loss: 0.8569\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.5981 - val_loss: 0.8719\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.5947 - val_loss: 0.8638\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.5950 - val_loss: 0.8775\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.5956 - val_loss: 0.8611\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.5989 - val_loss: 0.8570\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5944 - val_loss: 0.8810\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.5965 - val_loss: 0.8579\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.5990 - val_loss: 0.8672\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.5962 - val_loss: 0.8779\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.5964 - val_loss: 0.8458\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5960 - val_loss: 0.8595\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.5936 - val_loss: 0.8751\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 0.5936 - val_loss: 0.8713\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.5951 - val_loss: 0.8833\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.5975 - val_loss: 0.8690\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.5993 - val_loss: 0.8573\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 0.5960 - val_loss: 0.8681\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.5933 - val_loss: 0.8614\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5938 - val_loss: 0.8822\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.5939 - val_loss: 0.8668\n",
      "Epoch 412/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.5969 - val_loss: 0.8592\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6057 - val_loss: 0.9018\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6000 - val_loss: 0.8665\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6114 - val_loss: 0.8255\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.6133 - val_loss: 0.8316\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.6038 - val_loss: 0.8493\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.6031 - val_loss: 0.8692\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.5992 - val_loss: 0.8481\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.6051 - val_loss: 0.8487\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.6008 - val_loss: 0.8566\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.5976 - val_loss: 0.8668\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.5955 - val_loss: 0.8734\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.5975 - val_loss: 0.8542\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 1s 140ms/step - loss: 0.5986 - val_loss: 0.8610\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 0.5975 - val_loss: 0.8857\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.5960 - val_loss: 0.8719\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 0.5957 - val_loss: 0.8646\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.5940 - val_loss: 0.8700\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.5932 - val_loss: 0.8596\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.5927 - val_loss: 0.8696\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.5920 - val_loss: 0.8800\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.5941 - val_loss: 0.8582\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.5949 - val_loss: 0.8566\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.5935 - val_loss: 0.8889\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.6010 - val_loss: 0.8688\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.5972 - val_loss: 0.8505\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.5959 - val_loss: 0.8781\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.5951 - val_loss: 0.8628\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.5960 - val_loss: 0.8633\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.5918 - val_loss: 0.8777\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.5957 - val_loss: 0.8632\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.5966 - val_loss: 0.8605\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.5931 - val_loss: 0.8781\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.5925 - val_loss: 0.8650\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5920 - val_loss: 0.8675\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.5917 - val_loss: 0.8695\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.5926 - val_loss: 0.8708\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5946 - val_loss: 0.8856\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 57ms/step - loss: 0.5983 - val_loss: 0.8570\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5973 - val_loss: 0.8641\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.5941 - val_loss: 0.8770\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.5912 - val_loss: 0.8694\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5925 - val_loss: 0.8656\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.5917 - val_loss: 0.8718\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5915 - val_loss: 0.8634\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5925 - val_loss: 0.8704\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.5983 - val_loss: 0.8948\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.5985 - val_loss: 0.8491\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6062 - val_loss: 0.8442\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.5972 - val_loss: 0.8786\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.5951 - val_loss: 0.8813\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.5920 - val_loss: 0.8670\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.5929 - val_loss: 0.8674\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 53ms/step - loss: 0.5916 - val_loss: 0.8743\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.5912 - val_loss: 0.8751\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.5935 - val_loss: 0.8750\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.5952 - val_loss: 0.8652\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.5926 - val_loss: 0.8706\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5940 - val_loss: 0.8821\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5904 - val_loss: 0.8644\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.5914 - val_loss: 0.8656\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5927 - val_loss: 0.8697\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 0.5920 - val_loss: 0.8771\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5918 - val_loss: 0.8592\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5943 - val_loss: 0.8594\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.5912 - val_loss: 0.8824\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.5915 - val_loss: 0.8605\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5946 - val_loss: 0.8681\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5900 - val_loss: 0.8774\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5912 - val_loss: 0.8572\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5923 - val_loss: 0.8736\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5943 - val_loss: 0.8840\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.5909 - val_loss: 0.8659\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5910 - val_loss: 0.8722\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.5911 - val_loss: 0.8797\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.5908 - val_loss: 0.8532\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.5956 - val_loss: 0.8596\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5923 - val_loss: 0.8812\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5925 - val_loss: 0.8680\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5935 - val_loss: 0.8777\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.5919 - val_loss: 0.8734\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5909 - val_loss: 0.8635\n",
      "Epoch 494/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5900 - val_loss: 0.8743\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5909 - val_loss: 0.8736\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 0.5902 - val_loss: 0.8762\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.5893 - val_loss: 0.8739\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.5905 - val_loss: 0.8785\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.5903 - val_loss: 0.8775\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.5901 - val_loss: 0.8782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18ffd9f1c08>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = wave_multi_extra_layer_conditional()\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(1e-2), loss='mae')\n",
    "hist = model.fit(cond_gen, epochs=500, validation_data=test_cond_gen, verbose=1, callbacks=[make_tensorboard_callback('wave_multi_extra_layer_conditional')])\n",
    "# evaluate_model(model, hist)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.vstack([np.squeeze(model1.predict(test_data_gen)), test_gen_df[win_length:]]), index=['predict' , 'actual']).T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(np.vstack([np.squeeze(model1.predict(test_data_gen)), test_gen_df[win_length:]]), index=['predict' , 'actual']).T.apply(np.sign).sum(axis=1)==0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "93 / 173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.vstack([np.squeeze(model.predict(test_data_gen)), test_gen_df[win_length:]]), index=['predict' , 'actual']).T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(np.vstack([np.squeeze(model.predict(test_data_gen)), test_gen_df[win_length:]]), index=['predict' , 'actual']).T.apply(np.sign).sum(axis=1)==0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 24192), started 0:01:38 ago. (Use '!kill 24192' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dcec0dabc9f86abf\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dcec0dabc9f86abf\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
