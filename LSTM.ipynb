{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import *\n",
    "from pandas_datareader import data as pdr\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "yf.pdr_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  8 of 8 completed\n",
      "\n",
      "--- Full dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EURGBP=X</th>\n",
       "      <th>EURJPY=X</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>GBPJPY=X</th>\n",
       "      <th>GBPUSD=X</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>0.77616</td>\n",
       "      <td>144.779999</td>\n",
       "      <td>1.209863</td>\n",
       "      <td>186.429993</td>\n",
       "      <td>1.558094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>0.77590</td>\n",
       "      <td>144.929993</td>\n",
       "      <td>1.208941</td>\n",
       "      <td>186.764999</td>\n",
       "      <td>1.557972</td>\n",
       "      <td>2058.199951</td>\n",
       "      <td>2.123</td>\n",
       "      <td>17.790001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>0.78148</td>\n",
       "      <td>143.860001</td>\n",
       "      <td>1.194643</td>\n",
       "      <td>184.078003</td>\n",
       "      <td>1.528491</td>\n",
       "      <td>2020.579956</td>\n",
       "      <td>2.039</td>\n",
       "      <td>19.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>0.78240</td>\n",
       "      <td>142.557007</td>\n",
       "      <td>1.193902</td>\n",
       "      <td>182.205002</td>\n",
       "      <td>1.525832</td>\n",
       "      <td>2002.609985</td>\n",
       "      <td>1.963</td>\n",
       "      <td>21.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>0.78444</td>\n",
       "      <td>140.912003</td>\n",
       "      <td>1.187536</td>\n",
       "      <td>179.625000</td>\n",
       "      <td>1.513798</td>\n",
       "      <td>2025.900024</td>\n",
       "      <td>1.954</td>\n",
       "      <td>19.309999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>0.89660</td>\n",
       "      <td>125.769997</td>\n",
       "      <td>1.140394</td>\n",
       "      <td>140.169998</td>\n",
       "      <td>1.271084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>0.89949</td>\n",
       "      <td>126.001999</td>\n",
       "      <td>1.141553</td>\n",
       "      <td>140.052994</td>\n",
       "      <td>1.268875</td>\n",
       "      <td>2467.699951</td>\n",
       "      <td>2.797</td>\n",
       "      <td>30.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>0.89816</td>\n",
       "      <td>126.404999</td>\n",
       "      <td>1.136131</td>\n",
       "      <td>140.695999</td>\n",
       "      <td>1.264878</td>\n",
       "      <td>2488.830078</td>\n",
       "      <td>2.743</td>\n",
       "      <td>29.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>0.90386</td>\n",
       "      <td>126.767998</td>\n",
       "      <td>1.143105</td>\n",
       "      <td>140.205994</td>\n",
       "      <td>1.264622</td>\n",
       "      <td>2485.739990</td>\n",
       "      <td>2.736</td>\n",
       "      <td>28.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>0.90133</td>\n",
       "      <td>126.249001</td>\n",
       "      <td>1.143995</td>\n",
       "      <td>140.026993</td>\n",
       "      <td>1.269358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.686</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1043 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EURGBP=X    EURJPY=X  EURUSD=X    GBPJPY=X  GBPUSD=X        ^GSPC  \\\n",
       "Date                                                                            \n",
       "2015-01-01   0.77616  144.779999  1.209863  186.429993  1.558094          NaN   \n",
       "2015-01-02   0.77590  144.929993  1.208941  186.764999  1.557972  2058.199951   \n",
       "2015-01-05   0.78148  143.860001  1.194643  184.078003  1.528491  2020.579956   \n",
       "2015-01-06   0.78240  142.557007  1.193902  182.205002  1.525832  2002.609985   \n",
       "2015-01-07   0.78444  140.912003  1.187536  179.625000  1.513798  2025.900024   \n",
       "...              ...         ...       ...         ...       ...          ...   \n",
       "2018-12-25   0.89660  125.769997  1.140394  140.169998  1.271084          NaN   \n",
       "2018-12-26   0.89949  126.001999  1.141553  140.052994  1.268875  2467.699951   \n",
       "2018-12-27   0.89816  126.404999  1.136131  140.695999  1.264878  2488.830078   \n",
       "2018-12-28   0.90386  126.767998  1.143105  140.205994  1.264622  2485.739990   \n",
       "2018-12-31   0.90133  126.249001  1.143995  140.026993  1.269358          NaN   \n",
       "\n",
       "             ^TNX       ^VIX  \n",
       "Date                          \n",
       "2015-01-01    NaN        NaN  \n",
       "2015-01-02  2.123  17.790001  \n",
       "2015-01-05  2.039  19.920000  \n",
       "2015-01-06  1.963  21.120001  \n",
       "2015-01-07  1.954  19.309999  \n",
       "...           ...        ...  \n",
       "2018-12-25    NaN        NaN  \n",
       "2018-12-26  2.797  30.410000  \n",
       "2018-12-27  2.743  29.959999  \n",
       "2018-12-28  2.736  28.340000  \n",
       "2018-12-31  2.686        NaN  \n",
       "\n",
       "[1043 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Full dataset returns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EURGBP=X</th>\n",
       "      <th>EURJPY=X</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>GBPJPY=X</th>\n",
       "      <th>GBPUSD=X</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>0.000335</td>\n",
       "      <td>-0.001035</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>-0.001794</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>-0.007140</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>0.011968</td>\n",
       "      <td>0.014597</td>\n",
       "      <td>0.019288</td>\n",
       "      <td>0.018618</td>\n",
       "      <td>0.041197</td>\n",
       "      <td>-0.106928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>-0.001176</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.008973</td>\n",
       "      <td>0.038716</td>\n",
       "      <td>-0.056818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>-0.002601</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>0.005361</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>0.007950</td>\n",
       "      <td>-0.011496</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.093734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>-0.004870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>-0.003213</td>\n",
       "      <td>-0.001841</td>\n",
       "      <td>-0.001015</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>-0.003188</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>-0.004570</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>-0.008490</td>\n",
       "      <td>0.019686</td>\n",
       "      <td>0.015020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>-0.006306</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>-0.006101</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.057163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>-0.000777</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1043 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EURGBP=X  EURJPY=X  EURUSD=X  GBPJPY=X  GBPUSD=X     ^GSPC  \\\n",
       "Date                                                                     \n",
       "2015-01-01       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2015-01-02  0.000335 -0.001035  0.000762 -0.001794  0.000078       NaN   \n",
       "2015-01-05 -0.007140  0.007438  0.011968  0.014597  0.019288  0.018618   \n",
       "2015-01-06 -0.001176  0.009140  0.000621  0.010280  0.001742  0.008973   \n",
       "2015-01-07 -0.002601  0.011674  0.005361  0.014363  0.007950 -0.011496   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2018-12-25  0.002733  0.004150 -0.002729  0.002048 -0.004870       NaN   \n",
       "2018-12-26 -0.003213 -0.001841 -0.001015  0.000835  0.001741       NaN   \n",
       "2018-12-27  0.001481 -0.003188  0.004772 -0.004570  0.003159 -0.008490   \n",
       "2018-12-28 -0.006306 -0.002863 -0.006101  0.003495  0.000202  0.001243   \n",
       "2018-12-31  0.002807  0.004111 -0.000777  0.001278 -0.003731       NaN   \n",
       "\n",
       "                ^TNX      ^VIX  \n",
       "Date                            \n",
       "2015-01-01       NaN       NaN  \n",
       "2015-01-02       NaN       NaN  \n",
       "2015-01-05  0.041197 -0.106928  \n",
       "2015-01-06  0.038716 -0.056818  \n",
       "2015-01-07  0.004606  0.093734  \n",
       "...              ...       ...  \n",
       "2018-12-25       NaN       NaN  \n",
       "2018-12-26       NaN       NaN  \n",
       "2018-12-27  0.019686  0.015020  \n",
       "2018-12-28  0.002558  0.057163  \n",
       "2018-12-31  0.018615       NaN  \n",
       "\n",
       "[1043 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Train set unadjusted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EURGBP=X</th>\n",
       "      <th>EURJPY=X</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>GBPJPY=X</th>\n",
       "      <th>GBPUSD=X</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>-0.007140</td>\n",
       "      <td>0.007438</td>\n",
       "      <td>0.011968</td>\n",
       "      <td>0.014597</td>\n",
       "      <td>0.019288</td>\n",
       "      <td>0.018618</td>\n",
       "      <td>0.041197</td>\n",
       "      <td>-0.106928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>-0.001176</td>\n",
       "      <td>0.009140</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.008973</td>\n",
       "      <td>0.038716</td>\n",
       "      <td>-0.056818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>-0.002601</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>0.005361</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>0.007950</td>\n",
       "      <td>-0.011496</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.093734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>0.001455</td>\n",
       "      <td>-0.001736</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>-0.017574</td>\n",
       "      <td>-0.030754</td>\n",
       "      <td>0.135215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-09</th>\n",
       "      <td>0.002137</td>\n",
       "      <td>-0.000623</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>-0.002861</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.022831</td>\n",
       "      <td>-0.030769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>-0.004425</td>\n",
       "      <td>-0.005636</td>\n",
       "      <td>-0.003102</td>\n",
       "      <td>-0.001215</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>-0.001982</td>\n",
       "      <td>0.006449</td>\n",
       "      <td>0.010395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>-0.001339</td>\n",
       "      <td>-0.000628</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>-0.001610</td>\n",
       "      <td>-0.028283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>-0.021012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>-0.001430</td>\n",
       "      <td>-0.003689</td>\n",
       "      <td>-0.003605</td>\n",
       "      <td>-0.002392</td>\n",
       "      <td>-0.002180</td>\n",
       "      <td>-0.001831</td>\n",
       "      <td>-0.007401</td>\n",
       "      <td>0.028487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>-0.000585</td>\n",
       "      <td>-0.000378</td>\n",
       "      <td>-0.003427</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>-0.002748</td>\n",
       "      <td>0.005210</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>-0.077899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EURGBP=X  EURJPY=X  EURUSD=X  GBPJPY=X  GBPUSD=X     ^GSPC  \\\n",
       "Date                                                                     \n",
       "2015-01-05 -0.007140  0.007438  0.011968  0.014597  0.019288  0.018618   \n",
       "2015-01-06 -0.001176  0.009140  0.000621  0.010280  0.001742  0.008973   \n",
       "2015-01-07 -0.002601  0.011674  0.005361  0.014363  0.007950 -0.011496   \n",
       "2015-01-08  0.001455 -0.001736  0.003325 -0.002986  0.002013 -0.017574   \n",
       "2015-01-09  0.002137 -0.000623  0.003385 -0.002861  0.001299  0.008475   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-12-21 -0.004425 -0.005636 -0.003102 -0.001215  0.001111 -0.001982   \n",
       "2017-12-22  0.002370  0.001057  0.001758 -0.001339 -0.000628  0.000458   \n",
       "2017-12-27  0.000981  0.002093  0.001235  0.001050  0.000107 -0.000790   \n",
       "2017-12-28 -0.001430 -0.003689 -0.003605 -0.002392 -0.002180 -0.001831   \n",
       "2017-12-29 -0.000585 -0.000378 -0.003427  0.000257 -0.002748  0.005210   \n",
       "\n",
       "                ^TNX      ^VIX  \n",
       "Date                            \n",
       "2015-01-05  0.041197 -0.106928  \n",
       "2015-01-06  0.038716 -0.056818  \n",
       "2015-01-07  0.004606  0.093734  \n",
       "2015-01-08 -0.030754  0.135215  \n",
       "2015-01-09  0.022831 -0.030769  \n",
       "...              ...       ...  \n",
       "2017-12-21  0.006449  0.010395  \n",
       "2017-12-22 -0.001610 -0.028283  \n",
       "2017-12-27  0.021955 -0.021012  \n",
       "2017-12-28 -0.007401  0.028487  \n",
       "2017-12-29  0.011227 -0.077899  \n",
       "\n",
       "[718 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test set unadjusted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EURGBP=X</th>\n",
       "      <th>EURJPY=X</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>GBPJPY=X</th>\n",
       "      <th>GBPUSD=X</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>-0.004300</td>\n",
       "      <td>-0.001586</td>\n",
       "      <td>-0.006242</td>\n",
       "      <td>-0.006358</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.067760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>-0.001845</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.006281</td>\n",
       "      <td>-0.004012</td>\n",
       "      <td>-0.002446</td>\n",
       "      <td>-0.007592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>-0.001640</td>\n",
       "      <td>-0.006524</td>\n",
       "      <td>-0.004840</td>\n",
       "      <td>-0.004567</td>\n",
       "      <td>-0.003067</td>\n",
       "      <td>-0.006985</td>\n",
       "      <td>-0.009289</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.004151</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>-0.005019</td>\n",
       "      <td>-0.001694</td>\n",
       "      <td>-0.001660</td>\n",
       "      <td>-0.001613</td>\n",
       "      <td>-0.031513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>-0.001301</td>\n",
       "      <td>-0.025923</td>\n",
       "      <td>-0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-19</th>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>0.015636</td>\n",
       "      <td>0.016919</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>-0.003260</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>-0.001240</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.016025</td>\n",
       "      <td>-0.003944</td>\n",
       "      <td>-0.098661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>-0.002710</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>-0.005877</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>-0.003308</td>\n",
       "      <td>0.021021</td>\n",
       "      <td>-0.001075</td>\n",
       "      <td>-0.057456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>-0.003188</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>-0.004570</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>-0.008490</td>\n",
       "      <td>0.019686</td>\n",
       "      <td>0.015020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>-0.006306</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>-0.006101</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.057163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EURGBP=X  EURJPY=X  EURUSD=X  GBPJPY=X  GBPUSD=X     ^GSPC  \\\n",
       "Date                                                                     \n",
       "2018-01-03  0.001939  0.000288 -0.004300 -0.001586 -0.006242 -0.006358   \n",
       "2018-01-04 -0.001845  0.001235  0.004415  0.002991  0.006281 -0.004012   \n",
       "2018-01-05 -0.001640 -0.006524 -0.004840 -0.004567 -0.003067 -0.006985   \n",
       "2018-01-08  0.004151 -0.000595  0.002607 -0.005019 -0.001694 -0.001660   \n",
       "2018-01-09  0.005181  0.005678  0.005597  0.000580  0.000339 -0.001301   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2018-12-19  0.000768  0.000461 -0.002247 -0.000316 -0.003103  0.015636   \n",
       "2018-12-20 -0.003260 -0.000187 -0.001240  0.002994  0.002151  0.016025   \n",
       "2018-12-21 -0.002710  0.005230 -0.005877  0.007989 -0.003308  0.021021   \n",
       "2018-12-27  0.001481 -0.003188  0.004772 -0.004570  0.003159 -0.008490   \n",
       "2018-12-28 -0.006306 -0.002863 -0.006101  0.003495  0.000202  0.001243   \n",
       "\n",
       "                ^TNX      ^VIX  \n",
       "Date                            \n",
       "2018-01-03  0.007356  0.067760  \n",
       "2018-01-04 -0.002446 -0.007592  \n",
       "2018-01-05 -0.009289  0.000000  \n",
       "2018-01-08 -0.001613 -0.031513  \n",
       "2018-01-09 -0.025923 -0.055556  \n",
       "...              ...       ...  \n",
       "2018-12-19  0.016919  0.000000  \n",
       "2018-12-20 -0.003944 -0.098661  \n",
       "2018-12-21 -0.001075 -0.057456  \n",
       "2018-12-27  0.019686  0.015020  \n",
       "2018-12-28  0.002558  0.057163  \n",
       "\n",
       "[237 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- mu train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train set Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EURGBP=X</th>\n",
       "      <td>-0.000178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EURJPY=X</th>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EURUSD=X</th>\n",
       "      <td>-0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBPJPY=X</th>\n",
       "      <td>0.000305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBPUSD=X</th>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^GSPC</th>\n",
       "      <td>-0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^TNX</th>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^VIX</th>\n",
       "      <td>0.004274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Train set Mean\n",
       "EURGBP=X       -0.000178\n",
       "EURJPY=X        0.000090\n",
       "EURUSD=X       -0.000040\n",
       "GBPJPY=X        0.000305\n",
       "GBPUSD=X        0.000162\n",
       "^GSPC          -0.000278\n",
       "^TNX           -0.000102\n",
       "^VIX            0.004274"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- sigma train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train set st dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EURGBP=X</th>\n",
       "      <td>0.006369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EURJPY=X</th>\n",
       "      <td>0.006734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EURUSD=X</th>\n",
       "      <td>0.006126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBPJPY=X</th>\n",
       "      <td>0.008712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBPUSD=X</th>\n",
       "      <td>0.006712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^GSPC</th>\n",
       "      <td>0.007816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^TNX</th>\n",
       "      <td>0.021720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>^VIX</th>\n",
       "      <td>0.076458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Train set st dev\n",
       "EURGBP=X          0.006369\n",
       "EURJPY=X          0.006734\n",
       "EURUSD=X          0.006126\n",
       "GBPJPY=X          0.008712\n",
       "GBPUSD=X          0.006712\n",
       "^GSPC             0.007816\n",
       "^TNX              0.021720\n",
       "^VIX              0.076458"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Train set adjusted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EURGBP=X</th>\n",
       "      <th>EURJPY=X</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>GBPJPY=X</th>\n",
       "      <th>GBPUSD=X</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>-1.093199</td>\n",
       "      <td>1.091207</td>\n",
       "      <td>1.960396</td>\n",
       "      <td>1.640579</td>\n",
       "      <td>2.849289</td>\n",
       "      <td>2.417657</td>\n",
       "      <td>1.901404</td>\n",
       "      <td>-1.454415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>-0.156704</td>\n",
       "      <td>1.344024</td>\n",
       "      <td>0.107978</td>\n",
       "      <td>1.144988</td>\n",
       "      <td>0.235404</td>\n",
       "      <td>1.183619</td>\n",
       "      <td>1.787208</td>\n",
       "      <td>-0.799030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>-0.380396</td>\n",
       "      <td>1.720307</td>\n",
       "      <td>0.881667</td>\n",
       "      <td>1.613743</td>\n",
       "      <td>1.160138</td>\n",
       "      <td>-1.435322</td>\n",
       "      <td>0.216765</td>\n",
       "      <td>1.170050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>0.256436</td>\n",
       "      <td>-0.271073</td>\n",
       "      <td>0.549374</td>\n",
       "      <td>-0.377776</td>\n",
       "      <td>0.275753</td>\n",
       "      <td>-2.212937</td>\n",
       "      <td>-1.411214</td>\n",
       "      <td>1.712578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-09</th>\n",
       "      <td>0.363391</td>\n",
       "      <td>-0.105846</td>\n",
       "      <td>0.559180</td>\n",
       "      <td>-0.363456</td>\n",
       "      <td>0.169370</td>\n",
       "      <td>1.119871</td>\n",
       "      <td>1.055853</td>\n",
       "      <td>-0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>-0.666800</td>\n",
       "      <td>-0.850231</td>\n",
       "      <td>-0.499908</td>\n",
       "      <td>-0.174502</td>\n",
       "      <td>0.141348</td>\n",
       "      <td>-0.218008</td>\n",
       "      <td>0.301620</td>\n",
       "      <td>0.080056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>0.400024</td>\n",
       "      <td>0.143620</td>\n",
       "      <td>0.293546</td>\n",
       "      <td>-0.188687</td>\n",
       "      <td>-0.117818</td>\n",
       "      <td>0.094188</td>\n",
       "      <td>-0.069402</td>\n",
       "      <td>-0.425814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>0.182006</td>\n",
       "      <td>0.297562</td>\n",
       "      <td>0.208142</td>\n",
       "      <td>0.085585</td>\n",
       "      <td>-0.008242</td>\n",
       "      <td>-0.065575</td>\n",
       "      <td>1.015529</td>\n",
       "      <td>-0.330724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>-0.196684</td>\n",
       "      <td>-0.561159</td>\n",
       "      <td>-0.581918</td>\n",
       "      <td>-0.309621</td>\n",
       "      <td>-0.348980</td>\n",
       "      <td>-0.198679</td>\n",
       "      <td>-0.336050</td>\n",
       "      <td>0.316685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>-0.063998</td>\n",
       "      <td>-0.069514</td>\n",
       "      <td>-0.552963</td>\n",
       "      <td>-0.005486</td>\n",
       "      <td>-0.433551</td>\n",
       "      <td>0.702150</td>\n",
       "      <td>0.521580</td>\n",
       "      <td>-1.074741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EURGBP=X  EURJPY=X  EURUSD=X  GBPJPY=X  GBPUSD=X     ^GSPC  \\\n",
       "Date                                                                     \n",
       "2015-01-05 -1.093199  1.091207  1.960396  1.640579  2.849289  2.417657   \n",
       "2015-01-06 -0.156704  1.344024  0.107978  1.144988  0.235404  1.183619   \n",
       "2015-01-07 -0.380396  1.720307  0.881667  1.613743  1.160138 -1.435322   \n",
       "2015-01-08  0.256436 -0.271073  0.549374 -0.377776  0.275753 -2.212937   \n",
       "2015-01-09  0.363391 -0.105846  0.559180 -0.363456  0.169370  1.119871   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-12-21 -0.666800 -0.850231 -0.499908 -0.174502  0.141348 -0.218008   \n",
       "2017-12-22  0.400024  0.143620  0.293546 -0.188687 -0.117818  0.094188   \n",
       "2017-12-27  0.182006  0.297562  0.208142  0.085585 -0.008242 -0.065575   \n",
       "2017-12-28 -0.196684 -0.561159 -0.581918 -0.309621 -0.348980 -0.198679   \n",
       "2017-12-29 -0.063998 -0.069514 -0.552963 -0.005486 -0.433551  0.702150   \n",
       "\n",
       "                ^TNX      ^VIX  \n",
       "Date                            \n",
       "2015-01-05  1.901404 -1.454415  \n",
       "2015-01-06  1.787208 -0.799030  \n",
       "2015-01-07  0.216765  1.170050  \n",
       "2015-01-08 -1.411214  1.712578  \n",
       "2015-01-09  1.055853 -0.458333  \n",
       "...              ...       ...  \n",
       "2017-12-21  0.301620  0.080056  \n",
       "2017-12-22 -0.069402 -0.425814  \n",
       "2017-12-27  1.015529 -0.330724  \n",
       "2017-12-28 -0.336050  0.316685  \n",
       "2017-12-29  0.521580 -1.074741  \n",
       "\n",
       "[718 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test set adjusted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EURGBP=X</th>\n",
       "      <th>EURJPY=X</th>\n",
       "      <th>EURUSD=X</th>\n",
       "      <th>GBPJPY=X</th>\n",
       "      <th>GBPUSD=X</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>0.332323</td>\n",
       "      <td>0.029450</td>\n",
       "      <td>-0.695440</td>\n",
       "      <td>-0.217051</td>\n",
       "      <td>-0.954163</td>\n",
       "      <td>-0.777944</td>\n",
       "      <td>0.343372</td>\n",
       "      <td>0.830332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>-0.261786</td>\n",
       "      <td>0.170069</td>\n",
       "      <td>0.727342</td>\n",
       "      <td>0.308306</td>\n",
       "      <td>0.911624</td>\n",
       "      <td>-0.477831</td>\n",
       "      <td>-0.107909</td>\n",
       "      <td>-0.155201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>-0.229555</td>\n",
       "      <td>-0.982101</td>\n",
       "      <td>-0.783588</td>\n",
       "      <td>-0.559244</td>\n",
       "      <td>-0.481116</td>\n",
       "      <td>-0.858102</td>\n",
       "      <td>-0.422970</td>\n",
       "      <td>-0.055901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.679619</td>\n",
       "      <td>-0.101651</td>\n",
       "      <td>0.432117</td>\n",
       "      <td>-0.611164</td>\n",
       "      <td>-0.276592</td>\n",
       "      <td>-0.176793</td>\n",
       "      <td>-0.069552</td>\n",
       "      <td>-0.468057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>0.841426</td>\n",
       "      <td>0.829883</td>\n",
       "      <td>0.920332</td>\n",
       "      <td>0.031541</td>\n",
       "      <td>0.026388</td>\n",
       "      <td>-0.130944</td>\n",
       "      <td>-1.188795</td>\n",
       "      <td>-0.782515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-19</th>\n",
       "      <td>0.148456</td>\n",
       "      <td>0.055117</td>\n",
       "      <td>-0.360317</td>\n",
       "      <td>-0.071280</td>\n",
       "      <td>-0.486547</td>\n",
       "      <td>2.036132</td>\n",
       "      <td>0.783642</td>\n",
       "      <td>-0.055901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>-0.483991</td>\n",
       "      <td>-0.041169</td>\n",
       "      <td>-0.195848</td>\n",
       "      <td>0.308682</td>\n",
       "      <td>0.296314</td>\n",
       "      <td>2.085826</td>\n",
       "      <td>-0.176879</td>\n",
       "      <td>-1.346294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>-0.397515</td>\n",
       "      <td>0.763300</td>\n",
       "      <td>-0.952851</td>\n",
       "      <td>0.882040</td>\n",
       "      <td>-0.517086</td>\n",
       "      <td>2.725055</td>\n",
       "      <td>-0.044765</td>\n",
       "      <td>-0.807372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>0.260432</td>\n",
       "      <td>-0.486781</td>\n",
       "      <td>0.785531</td>\n",
       "      <td>-0.559604</td>\n",
       "      <td>0.446511</td>\n",
       "      <td>-1.050702</td>\n",
       "      <td>0.911073</td>\n",
       "      <td>0.140547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>-0.962245</td>\n",
       "      <td>-0.438566</td>\n",
       "      <td>-0.989422</td>\n",
       "      <td>0.366176</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.194592</td>\n",
       "      <td>0.122498</td>\n",
       "      <td>0.691737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EURGBP=X  EURJPY=X  EURUSD=X  GBPJPY=X  GBPUSD=X     ^GSPC  \\\n",
       "Date                                                                     \n",
       "2018-01-03  0.332323  0.029450 -0.695440 -0.217051 -0.954163 -0.777944   \n",
       "2018-01-04 -0.261786  0.170069  0.727342  0.308306  0.911624 -0.477831   \n",
       "2018-01-05 -0.229555 -0.982101 -0.783588 -0.559244 -0.481116 -0.858102   \n",
       "2018-01-08  0.679619 -0.101651  0.432117 -0.611164 -0.276592 -0.176793   \n",
       "2018-01-09  0.841426  0.829883  0.920332  0.031541  0.026388 -0.130944   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2018-12-19  0.148456  0.055117 -0.360317 -0.071280 -0.486547  2.036132   \n",
       "2018-12-20 -0.483991 -0.041169 -0.195848  0.308682  0.296314  2.085826   \n",
       "2018-12-21 -0.397515  0.763300 -0.952851  0.882040 -0.517086  2.725055   \n",
       "2018-12-27  0.260432 -0.486781  0.785531 -0.559604  0.446511 -1.050702   \n",
       "2018-12-28 -0.962245 -0.438566 -0.989422  0.366176  0.005975  0.194592   \n",
       "\n",
       "                ^TNX      ^VIX  \n",
       "Date                            \n",
       "2018-01-03  0.343372  0.830332  \n",
       "2018-01-04 -0.107909 -0.155201  \n",
       "2018-01-05 -0.422970 -0.055901  \n",
       "2018-01-08 -0.069552 -0.468057  \n",
       "2018-01-09 -1.188795 -0.782515  \n",
       "...              ...       ...  \n",
       "2018-12-19  0.783642 -0.055901  \n",
       "2018-12-20 -0.176879 -1.346294  \n",
       "2018-12-21 -0.044765 -0.807372  \n",
       "2018-12-27  0.911073  0.140547  \n",
       "2018-12-28  0.122498  0.691737  \n",
       "\n",
       "[237 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tickers\n",
    "tickers = ['^GSPC', '^VIX', '^TNX', 'EURUSD=X', 'EURJPY=X', 'GBPJPY=X', 'EURGBP=X', 'GBPUSD=X']\n",
    "\n",
    "# dates\n",
    "startdate = '2015-1-1'\n",
    "train_end = '2017-12-31'\n",
    "test_start = '2018-01-01'\n",
    "enddate = '2018-12-31'\n",
    "\n",
    "# data Close\n",
    "data_close = pdr.get_data_yahoo(tickers, start=startdate, end=enddate)['Adj Close']\n",
    "print('\\n--- Full dataset')\n",
    "display(data_close)\n",
    "\n",
    "# data returns\n",
    "data = data_close.shift(1) / data_close - 1\n",
    "print('\\n--- Full dataset returns')\n",
    "display(data)\n",
    "\n",
    "# Sets\n",
    "data_train = data[startdate:train_end].dropna()\n",
    "data_test = data[test_start:enddate].dropna()\n",
    "print('\\n--- Train set unadjusted')\n",
    "display(data_train)\n",
    "print('\\n--- Test set unadjusted')\n",
    "display(data_test)\n",
    "\n",
    "# mu sigma train\n",
    "mu_train = data_train.mean()\n",
    "sigma_train = data_train.std()\n",
    "print('\\n--- mu train:')\n",
    "display(pd.DataFrame(mu_train, columns=['Train set Mean']))\n",
    "print('\\n--- sigma train:')\n",
    "display(pd.DataFrame(sigma_train, columns=['Train set st dev']))\n",
    "\n",
    "# Final sets\n",
    "data_train_prod = (data_train - mu_train) / sigma_train\n",
    "data_test_prod = (data_test - mu_train) / sigma_train\n",
    "print('\\n--- Train set adjusted')\n",
    "display(data_train_prod)\n",
    "print('\\n--- Test set adjusted')\n",
    "display(data_test_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_std = data_train_prod.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=3\n",
    "num_timesteps=1\n",
    "lstm_model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_timesteps, num_features)),\n",
    "    tf.keras.layers.LSTM(25, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "num_timesteps=16\n",
    "lstm_model_16 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_timesteps, num_features)),\n",
    "    tf.keras.layers.LSTM(25, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "num_timesteps=256\n",
    "lstm_model_256 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_timesteps, num_features)),\n",
    "    tf.keras.layers.LSTM(25, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EURGBP=X', 'EURJPY=X', 'EURUSD=X', 'GBPJPY=X', 'GBPUSD=X', '^GSPC', '^TNX', '^VIX']\n"
     ]
    }
   ],
   "source": [
    "print(list(data_train_prod.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Forecasting (Conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = data_train_prod.loc[:, ['^GSPC', '^TNX', '^VIX']]\n",
    "idx_test = data_test_prod.loc[:, ['^GSPC', '^TNX', '^VIX']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 25)                2900      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 2,926\n",
      "Trainable params: 2,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model_1.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "lstm_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 1\n",
    "num_features=3\n",
    "inputs_1 = idx_train.values[:-window, :].reshape(-1, window, num_features)\n",
    "labels_1 = idx_train.values[window:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6848\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6810\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6782\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6776\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6764\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6754\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6758\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6766\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6753\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6762\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6746\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6747\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6754\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6734\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6731\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6738\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6751\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6756\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6755\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6736\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.655 - 0s 4ms/step - loss: 0.6741\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6748\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6747\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6738\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6745\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6737\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6725\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6734\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6740\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6744\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6732\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6720\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6735\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6725\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6726\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6729\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6735\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6733\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6739\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6718\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6713\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6714\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6731\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6727\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6712A: 0s - loss: 0.677\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6728\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6720\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6705\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6731\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x214f5728470>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_1.fit(inputs_1, labels_1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 1\n",
    "num_features=3\n",
    "inputs_1_test = idx_test.values[:-window, :].reshape(-1, window, num_features)\n",
    "labels_1_test = idx_test.values[window:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 0.9178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9177559614181519"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_1.evaluate(inputs_1_test, labels_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_data(data, window_size):\n",
    "    last_index = len(data)-window_size\n",
    "    \n",
    "    trunc_data = [data[i:i+window_size] for i in range(last_index)]\n",
    "    return np.dstack(trunc_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 25)                2900      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 2,926\n",
      "Trainable params: 2,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model_16.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "lstm_model_16.summary()\n",
    "window = 16\n",
    "num_features=3\n",
    "inputs_16 = window_data(idx_train.values, window)\n",
    "inputs_16 = np.transpose(inputs_16, (2, 0, 1))\n",
    "labels_16 = idx_train.values[window:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6751\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6689\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6685\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6659\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6622\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6629\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6628\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6575\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6614\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.657 - 0s 8ms/step - loss: 0.6587\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6569\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6557\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6566\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6562\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6549\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6529\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6507\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6531\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6500\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6535\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6501\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6531\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6470\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6452\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6498\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6450\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6463\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6439\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6437\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6422\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6410\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6365\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6357\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6361A: 0s - loss: 0.6\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6330\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6317\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6337\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.634 - 0s 7ms/step - loss: 0.6344\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6324\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6324\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6254\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6297\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6241\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6244\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6285\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6311\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6226\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6293\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6186\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2149d2db0b8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_16.fit(inputs_16, labels_16, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 0.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9875006079673767"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 16\n",
    "num_features=3\n",
    "inputs_16_test = window_data(idx_test.values, window)\n",
    "inputs_16_test = np.transpose(inputs_16_test, (2, 0, 1))\n",
    "labels_16_test = idx_test.values[window:, 0]\n",
    "lstm_model_16.evaluate(inputs_16_test, labels_16_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 25)                2900      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 2,926\n",
      "Trainable params: 2,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model_256.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "lstm_model_256.summary()\n",
    "window = 256\n",
    "num_features=3\n",
    "inputs_256 = window_data(idx_train.values, window)\n",
    "inputs_256 = np.transpose(inputs_256, (2, 0, 1))\n",
    "labels_256 = idx_train.values[window:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.5292\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.5236\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.5221\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.5186\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5185\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5187\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.5211\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.5178\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.5163: 0s - los\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5121\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.5155\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.5140\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.5113\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.5133\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.5124\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5121\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.5080\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.5113\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.5086\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.5062\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.5052\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.5075\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.5071\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.5087\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.5038\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.5051\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5051\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.5023\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.5063\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.4995\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.5033: 0s - loss: 0.56 - ETA: 0s - loss:\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.5024\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.5023\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4991\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4992\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.5005\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4977\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.4981\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4975\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4914\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4926\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4933\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4913\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.4929\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.4916\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4901\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4891\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4916\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.4973\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.4948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2149f645860>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model_256.fit(inputs_256, labels_256, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = 256\n",
    "# num_features=3\n",
    "# inputs_256_test = window_data(idx_test.values, window)\n",
    "# inputs_256_test = np.transpose(inputs_256_test, (2, 0, 1))\n",
    "# labels_256_test = idx_test.values[window:, 0]\n",
    "# lstm_model_256.evaluate(inputs_256_test, labels_256_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Forecasting (Unconditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Forecasting Classification (Conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=3\n",
    "num_timesteps=1\n",
    "class_lstm_model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_timesteps, num_features)),\n",
    "    tf.keras.layers.LSTM(25, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "num_timesteps=16\n",
    "class_lstm_model_16 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_timesteps, num_features)),\n",
    "    tf.keras.layers.LSTM(25, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "num_timesteps=256\n",
    "class_lstm_model_256 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_timesteps, num_features)),\n",
    "    tf.keras.layers.LSTM(25, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 25)                2900      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 2,926\n",
      "Trainable params: 2,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class_lstm_model_1.compile(loss=\"mae\", optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "class_lstm_model_1.summary()\n",
    "window = 1\n",
    "num_features=3\n",
    "inputs_1 = window_data(idx_train.values, window)\n",
    "inputs_1 = np.transpose(inputs_1, (2, 0, 1))\n",
    "#idx_train.values[:-window, :].reshape(-1, window, num_features)\n",
    "labels_1 = idx_train.values[window:, 0] > 0\n",
    "labels_1 = labels_1.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4699 - binary_accuracy: 0.5495\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4681 - binary_accuracy: 0.5509\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4683 - binary_accuracy: 0.5523\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4674 - binary_accuracy: 0.5523\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4658 - binary_accuracy: 0.5467\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4652 - binary_accuracy: 0.5551\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4652 - binary_accuracy: 0.5495\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4652 - binary_accuracy: 0.5537\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4645 - binary_accuracy: 0.5509\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4658 - binary_accuracy: 0.5495\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4631 - binary_accuracy: 0.5509\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4645 - binary_accuracy: 0.5551\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4634 - binary_accuracy: 0.5523\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4614 - binary_accuracy: 0.5579\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4619 - binary_accuracy: 0.5509\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4611 - binary_accuracy: 0.5565\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4598 - binary_accuracy: 0.5537\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4607 - binary_accuracy: 0.5537\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4612 - binary_accuracy: 0.5565\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4586 - binary_accuracy: 0.5551\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4584 - binary_accuracy: 0.5537\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4588 - binary_accuracy: 0.5537\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4586 - binary_accuracy: 0.5579\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4579 - binary_accuracy: 0.5565\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4592 - binary_accuracy: 0.5551\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4575 - binary_accuracy: 0.5565\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4578 - binary_accuracy: 0.5579\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4576 - binary_accuracy: 0.5537\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4568 - binary_accuracy: 0.5565\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4576 - binary_accuracy: 0.5579\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4552 - binary_accuracy: 0.5593\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4556 - binary_accuracy: 0.5635\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4550 - binary_accuracy: 0.5593\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4579 - binary_accuracy: 0.554 - 0s 2ms/step - loss: 0.4553 - binary_accuracy: 0.5579\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4551 - binary_accuracy: 0.5621\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4545 - binary_accuracy: 0.5593\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4554 - binary_accuracy: 0.5565\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4557 - binary_accuracy: 0.5551\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4532 - binary_accuracy: 0.5649\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4534 - binary_accuracy: 0.5607\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4542 - binary_accuracy: 0.5621\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4517 - binary_accuracy: 0.5635\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4529 - binary_accuracy: 0.5635\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4531 - binary_accuracy: 0.5649\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4512 - binary_accuracy: 0.5649\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4509 - binary_accuracy: 0.5621\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.4526 - binary_accuracy: 0.5662\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4515 - binary_accuracy: 0.5649\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4515 - binary_accuracy: 0.5635\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4506 - binary_accuracy: 0.5649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x214e94bd208>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_lstm_model_1.fit(inputs_1, labels_1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5261 - binary_accuracy: 0.4661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5261451005935669, 0.4661017060279846]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 1\n",
    "num_features=3\n",
    "inputs_1_test = window_data(idx_test.values, window)\n",
    "inputs_1_test = np.transpose(inputs_1_test, (2, 0, 1))\n",
    "labels_1_test = idx_test.values[window:, 0] > 0\n",
    "labels_1_test = labels_1_test.astype(int)\n",
    "class_lstm_model_1.evaluate(inputs_1_test, labels_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 25)                2900      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 2,926\n",
      "Trainable params: 2,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class_lstm_model_16.compile(loss=\"mae\", optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "class_lstm_model_16.summary()\n",
    "window = 16\n",
    "num_features=3\n",
    "inputs_16 = window_data(idx_train.values, window)\n",
    "inputs_16 = np.transpose(inputs_16, (2, 0, 1))\n",
    "#idx_train.values[:-window, :].reshape(-1, window, num_features)\n",
    "labels_16 = idx_train.values[window:, 0] > 0\n",
    "labels_16 = labels_16.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3852 - binary_accuracy: 0.6211\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3766 - binary_accuracy: 0.6296\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3760 - binary_accuracy: 0.6325\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3718 - binary_accuracy: 0.6368\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3682 - binary_accuracy: 0.6467\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3701 - binary_accuracy: 0.6368\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3712 - binary_accuracy: 0.6368A: 0s - loss: 0.3712 - binary_accuracy: 0.636\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3686 - binary_accuracy: 0.6353\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3670 - binary_accuracy: 0.6382\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3782 - binary_accuracy: 0.6282\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3685 - binary_accuracy: 0.6339\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3656 - binary_accuracy: 0.6382\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3648 - binary_accuracy: 0.6425\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3724 - binary_accuracy: 0.6368\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3881 - binary_accuracy: 0.6154\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3790 - binary_accuracy: 0.6254A: 0s - loss: 0.4315 - binary_accuracy: 0.5\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3606 - binary_accuracy: 0.6481\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3670 - binary_accuracy: 0.6368\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3683 - binary_accuracy: 0.6382\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3698 - binary_accuracy: 0.6339\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3631 - binary_accuracy: 0.6410\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3692 - binary_accuracy: 0.6353\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3731 - binary_accuracy: 0.6325\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3588 - binary_accuracy: 0.6496\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3614 - binary_accuracy: 0.6439\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3652 - binary_accuracy: 0.6410A: 0s - loss: 0.3455 - binary_accuracy: 0.6\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3648 - binary_accuracy: 0.6439\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3619 - binary_accuracy: 0.6410\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3604 - binary_accuracy: 0.6453\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3578 - binary_accuracy: 0.6510\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3593 - binary_accuracy: 0.6425\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3560 - binary_accuracy: 0.6510\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3497 - binary_accuracy: 0.663 - 0s 8ms/step - loss: 0.3536 - binary_accuracy: 0.6595\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3581 - binary_accuracy: 0.6453\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3562 - binary_accuracy: 0.6481\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3494 - binary_accuracy: 0.6610\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3516 - binary_accuracy: 0.6538\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3483 - binary_accuracy: 0.6638\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3470 - binary_accuracy: 0.6595\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3482 - binary_accuracy: 0.6610\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.3582 - binary_accuracy: 0.6453\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3723 - binary_accuracy: 0.6296\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3543 - binary_accuracy: 0.6524\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3542 - binary_accuracy: 0.6481\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3498 - binary_accuracy: 0.6538\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3480 - binary_accuracy: 0.6581\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.3515 - binary_accuracy: 0.6538\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3592 - binary_accuracy: 0.6396\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3457 - binary_accuracy: 0.6610\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.3422 - binary_accuracy: 0.6652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x214eb81dd30>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_lstm_model_16.fit(inputs_16, labels_16, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4846 - binary_accuracy: 0.5158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.484597772359848, 0.5158371329307556]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 16\n",
    "num_features=3\n",
    "inputs_16_test = window_data(idx_test.values, window)\n",
    "inputs_16_test = np.transpose(inputs_16_test, (2, 0, 1))\n",
    "labels_16_test = idx_test.values[window:, 0] > 0\n",
    "labels_16_test = labels_16_test.astype(int)\n",
    "class_lstm_model_16.evaluate(inputs_16_test, labels_16_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 25)                2900      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 2,926\n",
      "Trainable params: 2,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class_lstm_model_256.compile(loss=\"mae\", optimizer=\"adam\", metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "class_lstm_model_256.summary()\n",
    "window = 256\n",
    "num_features=3\n",
    "inputs_256 = window_data(idx_train.values, window)\n",
    "inputs_256 = np.transpose(inputs_256, (2, 0, 1))\n",
    "#idx_train.values[:-window, :].reshape(-1, window, num_features)\n",
    "labels_256 = idx_train.values[window:, 0] > 0\n",
    "labels_256 = labels_256.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.3633 - binary_accuracy: 0.6450: 0s - loss: 0.3677 - binary_accuracy: 0.\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.3447 - binary_accuracy: 0.6905\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.3495 - binary_accuracy: 0.6775\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.3493 - binary_accuracy: 0.6840\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.3499 - binary_accuracy: 0.6775\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.3559 - binary_accuracy: 0.6623\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.3482 - binary_accuracy: 0.6688\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.3589 - binary_accuracy: 0.6515\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.3407 - binary_accuracy: 0.6861\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.3370 - binary_accuracy: 0.6905\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.3421 - binary_accuracy: 0.6818\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.3396 - binary_accuracy: 0.6883\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.3372 - binary_accuracy: 0.6948: 0s - loss: 0.3525 - binary_ac\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.3378 - binary_accuracy: 0.6753\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.3379 - binary_accuracy: 0.6818\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.3374 - binary_accuracy: 0.6970\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.3426 - binary_accuracy: 0.6840\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.3311 - binary_accuracy: 0.6926: 0s - loss: 0.3287 - binary_accuracy: 0.\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.3356 - binary_accuracy: 0.6883\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.3387 - binary_accuracy: 0.6905\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.3544 - binary_accuracy: 0.6710\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.3494 - binary_accuracy: 0.6710\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.3393 - binary_accuracy: 0.6797\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.3418 - binary_accuracy: 0.6818\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.3313 - binary_accuracy: 0.7035\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.3405 - binary_accuracy: 0.6948\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.3419 - binary_accuracy: 0.6732\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.3449 - binary_accuracy: 0.6732\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.3343 - binary_accuracy: 0.6861: 0s - loss: 0.3343 - binary_accuracy: 0.686\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.3316 - binary_accuracy: 0.6905\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.3359 - binary_accuracy: 0.6840\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.3303 - binary_accuracy: 0.6948\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.3288 - binary_accuracy: 0.6840\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.3413 - binary_accuracy: 0.6732\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.3436 - binary_accuracy: 0.6732\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.3496 - binary_accuracy: 0.6645\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.3355 - binary_accuracy: 0.6732\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.3401 - binary_accuracy: 0.6883\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.3322 - binary_accuracy: 0.6818\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.3237 - binary_accuracy: 0.6926\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.3230 - binary_accuracy: 0.7056\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.3221 - binary_accuracy: 0.7035\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.3209 - binary_accuracy: 0.6991\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.3274 - binary_accuracy: 0.6970\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.3189 - binary_accuracy: 0.6991\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.3221 - binary_accuracy: 0.6991\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.3284 - binary_accuracy: 0.6948\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.3281 - binary_accuracy: 0.6905\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.3264 - binary_accuracy: 0.6991\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.3198 - binary_accuracy: 0.7056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x214edb877b8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_lstm_model_256.fit(inputs_256, labels_256, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Forecasting Classification (Unconditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exchange Rate Forecasting (Conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=5\n",
    "num_timesteps=1\n",
    "er_lstm_model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_timesteps, num_features)),\n",
    "    tf.keras.layers.LSTM(25, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "num_timesteps=16\n",
    "er_lstm_model_16 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_timesteps, num_features)),\n",
    "    tf.keras.layers.LSTM(25, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "num_timesteps=256\n",
    "er_lstm_model_256 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(num_timesteps, num_features)),\n",
    "    tf.keras.layers.LSTM(25, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_columns = ['EURGBP=X', 'EURJPY=X', 'EURUSD=X', 'GBPJPY=X', 'GBPUSD=X']\n",
    "er_train = data_train_prod.loc[:, er_columns]\n",
    "er_test = data_test_prod.loc[:, er_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURGBP=X\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7484\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7466\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7478\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7440\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7455\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7416\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7412\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7424\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7426\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7440\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7427\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7431\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7368\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7388\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7365\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7369\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7397\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7362\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7358\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7369\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7361\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7373\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7356\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7346\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7340\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7349\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7359\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7332\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7342\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7332\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7348\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7314\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7339\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7341\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7358\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7305\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7323\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7327\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7299\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7353\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7319\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7287\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7354\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7340\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7338\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7324\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7340\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7313\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7334\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7320\n",
      "EURJPY=X\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7181\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7145\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7165\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7168\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7123\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7119\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7101\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7126\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7090\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7057\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7065\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7101\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7053\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7117\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7066\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7072\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7071\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7092\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7088\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7048\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7101\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7046\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7063\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7090\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7062\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7049\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7047\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7069\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7064\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7089\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7099\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7065\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7064\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7042\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7057\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7051\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7048\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7040\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7045\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7044\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7065\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7055\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7101\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7095\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7038\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7046\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7061\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7021\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7028\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7080\n",
      "EURUSD=X\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7392\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7350\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7297\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7299\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7280\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7344\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7280\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7263\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7255\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7278\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7232\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7223\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7243\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7304\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7254\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7230\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7232\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7229\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7229\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7219\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7240\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7219\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7210\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7264\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7227\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7245\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7231\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7247\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7242\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7210\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7213\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7184\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7232\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7215\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7241\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7246\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7242\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7210\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7184\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7267\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7200\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7243\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7234\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7224\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7225\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7208\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7262\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7188\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.7197\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.7180\n",
      "GBPJPY=X\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6701\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6645\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6598\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6556\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6540\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6528\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6521\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6560\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6515\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6456\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6469\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6393\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6499\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6459\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6486\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6504\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6470\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6479\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6464\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6450\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6484\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6449\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6451\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6453\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6477\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6483\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6457\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6464\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6481\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6478\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6465\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6487\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6448\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6445\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6482\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6458\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6472\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6460\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6449\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6413\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6477\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6430\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6453\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6432\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6434\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6463\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6451\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6467\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6430\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.6411\n",
      "GBPUSD=X\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6781\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6806\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6731\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6725\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6746\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6701\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6697\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6713\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6698\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6662\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6710\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6699\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6715\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6708\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6700\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6662\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6718\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6730\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6675\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6680\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6695\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6667\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6692\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6715\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6682\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6670\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6714\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6682\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6707\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6700\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6730\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6699\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6690\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6663\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6674\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6644\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6682\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6707\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6671\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6647\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6651\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6673\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6696\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6684\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6687\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6683\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6723\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6671\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6674\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6667\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(er_columns[i])\n",
    "    er_lstm_model_1.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "    window = 1\n",
    "    num_features=5\n",
    "    inputs_1 = er_train.values[:-window, :].reshape(-1, window, num_features)\n",
    "    labels_1 = er_train.values[window:, i]\n",
    "    er_lstm_model_1.fit(inputs_1, labels_1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5336290001869202"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 1\n",
    "num_features=5\n",
    "er_inputs_1_test = window_data(er_test.values, window)\n",
    "er_inputs_1_test = np.transpose(er_inputs_1_test, (2, 0, 1))\n",
    "er_labels_1_test = er_test.values[window:, 0] > 0\n",
    "er_labels_1_test = er_labels_1_test.astype(int)\n",
    "er_lstm_model_1.evaluate(er_inputs_1_test, er_labels_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURGBP=X\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6797\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.6734\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6676\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6674\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6623\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6642\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6659\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6640\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6643\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6637\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6615\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6625\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6605\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6645\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6615\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6577\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6559\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6582\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6546\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6552\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6520\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6614\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6513\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6562\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6497\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6544\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6497\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6530\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6527\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6451\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6516\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6477\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6463\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6464\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6475\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6426\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6451\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6427\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6370A: 0s - loss: 0.6\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6382\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6387\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6402\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6449\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6380\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6324\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6369\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6326\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6359\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6329\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6315\n",
      "EURJPY=X\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6375\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6317\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6242\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6301\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6291\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6278\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6281\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6325\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6278\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6220\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6269\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6216\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6262\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6231\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6260\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6276\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6211\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6205\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6212\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6219\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6184\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6239\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6218\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6266\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6202\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6135\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6155\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6156\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6165\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6148\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6126\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6100\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6129\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6074\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6142\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.6028\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6072\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6118\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6060\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6105\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6018\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6012\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6022\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6048\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5954\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5928\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6046\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6005A: 0s - loss: 0.58\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5966\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5970\n",
      "EURUSD=X\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5942\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5944\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5945\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5902\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5947A: 0s - loss: 0.6\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5887\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5875\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5881\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5946\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5958\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5874\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5872\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5881\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5779\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5914\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5918\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5891\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5842\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5866\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5858\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5795\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5791\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5864\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5762\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5838\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5790\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5763\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5832\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5807\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5729\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5709\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5731\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5716\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5747\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5719\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5688\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5694\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.569 - 0s 8ms/step - loss: 0.5653\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5667\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5701\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5755\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5641\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5635\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5595\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5647\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5638\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5659\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5695\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5732\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5677\n",
      "GBPJPY=X\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5632\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5553\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5547\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5529\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5562\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5524\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5544\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5492\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5455\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5460\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5485\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5539\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5551\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5574\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5407\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5545\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5463\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5524\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5484\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5545\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5368\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5427\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5419\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5417\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5458\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5397\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5400\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5329\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5504\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5380\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5454\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5393\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5258\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5375\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5275\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5451\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5305\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5175\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5255\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5336\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5282\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5300\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5233\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5280\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5251\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5078\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5176\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5265\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5196\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5090\n",
      "GBPUSD=X\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5107\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5178\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5133\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5221\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5026\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5068\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5051\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5112\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5102\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5018\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5096\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4951\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5135\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5043\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5117\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5130\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5035\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5039\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5065\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5144\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4999\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5154\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5084\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5048\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5113\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5004\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5041\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4843\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4928\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4925- ETA: 0s - loss: 0.5 - 0s 7ms/step - loss: 0.4925\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4980\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4974\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4930\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4932\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4949\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4851\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4937\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4968\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4972\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.4775\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4953\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4936\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4877\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4794\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4690A: 0s - loss: 0.473\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4850\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4880\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4740\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4753\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4791\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(er_columns[i])\n",
    "    er_lstm_model_16.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "    window = 16\n",
    "    num_features=5\n",
    "    inputs_16 = window_data(er_train.values, window)\n",
    "    inputs_16 = np.transpose(inputs_16, (2, 0, 1))\n",
    "    labels_16 = er_train.values[window:, i]\n",
    "    er_lstm_model_16.fit(inputs_16, labels_16, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EURGBP=X\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.9249\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.8334\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.8017: 0s - loss: 0.\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7861\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.7845\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.7742\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.7706\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.7730\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.7608\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.7691\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 2s 100ms/step - loss: 0.7504\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.7587\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.7559\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.7387\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.7442\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.7473\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.7349\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.7230\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.7238\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.7282: 0s - l\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.7381\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7315\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.7317\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.7525\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.7564: 0s \n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.7482\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.7311\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.7312\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.7313\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7242\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.7177\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.7156\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.7213\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.7193\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.7140\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.7189\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7123\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.7090\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.7111\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.7095\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.7068\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.7095\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.7053\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6994\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.7045\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.7008\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.7028\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6998\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.7002\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.7025\n",
      "EURJPY=X\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.6929\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.6751\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6801\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.6669\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.6723\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.6701\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6713\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.6567\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.6657\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.6582\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.6520\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.6513\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.6466\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.6504\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.6493\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.6476\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6412\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.6386\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.6428\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.6352\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.6329\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6365\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6291\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.6272\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.6288\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6350\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6258\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6235\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6213\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.6246\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.6154\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6139\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6116\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6003\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.6041\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6068\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.5962\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6039\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.6122\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6100\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5956\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.6035\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.5913\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.5934\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.5934\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.5879\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.5997\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.5883\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.5882\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.5797\n",
      "EURUSD=X\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.6284\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.6101\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.5903\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.5907\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.5741: 0s - loss: 0.5\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.5780\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.5715\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.5638\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.5665\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.5629\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.5518\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.5514\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.5514\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.5464\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.5480\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.5434\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5462\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.5562\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5398\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.5473\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5316\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5380\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.5396\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.5264\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.5331\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5343\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.5381\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.5230\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5305\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.5198\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5264\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.5202\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.5123\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.5232\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5158\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.5245\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.5136\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.5016\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.5048\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.5060\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.5121\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.5019\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.5019\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.5048\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.5015\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5023\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.5021\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.5082\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.5006\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.4902\n",
      "GBPJPY=X\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.7547\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.7320\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.7096\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.7049\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.7016\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.7002\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6963: 0s - loss: 0.7\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.6888\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6839: 0s - loss\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.6814\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.6781\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.6755\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.6737\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.6717\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.6677\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.6659\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.6573\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.6582\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6542\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.6533: 0s - \n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.6588\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.6469\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6432: 0s -\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.6485\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6483\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.6434\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6399\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.6288\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6262\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.6306: 0s \n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.6274\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.6269\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.6248\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.6256: 0s \n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6187\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6177: 0s - loss: \n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6249\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.6179\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.6188\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.6123\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.6129\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.6105\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.6080\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.6198\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.6011\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.6056\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.5911\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.6045\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.5951\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.5829\n",
      "GBPUSD=X\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.6716\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.6571\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.6437\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6369\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.6316\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.6246\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.6329\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.6339\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.6056\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.6142\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.6052\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.6051\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.6016\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 0.6123\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.6075\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.5873\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 78ms/step - loss: 0.6037\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.5978\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.5991\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.5892\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.5968\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.5808\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 2s 101ms/step - loss: 0.5897\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.5821\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5790\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.5819: 0s - lo\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 1s 79ms/step - loss: 0.5808\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.5762: 0s - loss: 0.57\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - ETA: 0s - loss: 0.585 - 1s 79ms/step - loss: 0.5858\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.5690\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 1s 87ms/step - loss: 0.5699\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5639\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.5807\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.5804\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 1s 90ms/step - loss: 0.5798\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 1s 89ms/step - loss: 0.5558\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 1s 88ms/step - loss: 0.5783\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.5583\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.5728\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.5602\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5550\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.5526\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 0.5522\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.5598\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.5502\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.5490\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.5509: 0s \n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.5629\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.5459\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.5597\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(er_columns[i])\n",
    "    er_lstm_model_256.compile(loss=\"mae\", optimizer=\"adam\")\n",
    "    window = 256\n",
    "    num_features=5\n",
    "    inputs_256 = window_data(er_train.values, window)\n",
    "    inputs_256 = np.transpose(inputs_256, (2, 0, 1))\n",
    "    labels_256 = er_train.values[window:, i]\n",
    "    er_lstm_model_256.fit(inputs_256, labels_256, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
